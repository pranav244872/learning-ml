{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Feature Scaling\n",
        "\n",
        "Feature scaling is a technique to standardize the range of independent variables or features of data. In this example, we consider two features:\n",
        "\n",
        "- $ x_1 $: size (feet$^2$), ranging from 300 to 2000\n",
        "- $ x_2 $: number of bedrooms, ranging from 0 to 5\n",
        "\n",
        "Suppose the price is determined by the linear model:\n",
        "\n",
        " $$ text{price} = w_1 x_1 + w_2 x_2 + b$$\n",
        "\n",
        "#### Example 1: Incorrect Parameters\n",
        "\n",
        "Given:\n",
        "- $ x_1 = 2000$\n",
        "- $ x_2 = 5$\n",
        "- $\\text{price} = \\$500k$\n",
        "\n",
        "Assume:\n",
        "- $ w_1 = 50$\n",
        "- $ w_2 = 0.1$\n",
        "- $ b = 50$\n",
        "\n",
        "Then:\n",
        "\n",
        "$$ \\text{price} = 50 \\cdot 2000 + 0.1 \\cdot 5 + 50 = 100050.5 \\, \\text{k}$$\n",
        "\n",
        "This is far from the actual price of \\$500k, indicating that these parameters are not a good fit.\n",
        "\n",
        "#### Example 2: Better Parameters\n",
        "\n",
        "Assume:\n",
        "- $ w_1 = 0.1$\n",
        "- $ w_2 = 50$\n",
        "- $ b = 50$\n",
        "\n",
        "Then:\n",
        "\n",
        "$$ \\text{price} = 0.1 \\cdot 2000 + 50 \\cdot 5 + 50 = 500 \\, \\text{k}$$\n",
        "\n",
        "This matches the actual price, showing that a good model requires smaller weights for large-range features.\n",
        "\n",
        "#### Importance of Feature Scaling\n",
        "\n",
        "- **Effect on Gradient Descent**: When plotting $ x_1 $ vs. $ x_2 $, we observe that the horizontal scale (size) has a much larger range of values compared to the vertical scale (number of bedrooms). In a contour plot of $ w_1 $ vs. $ w_2 $, this results in a vertical ellipse, where $ w_1 $ ranges from 0 to 1 and $ w_2 $ from 10 to 100. This indicates that small changes in $ w_1 $ significantly impact the cost $ J $, while small changes in $ w_2 $ do not.\n",
        "\n",
        "- **Gradient Descent Efficiency**: Without feature scaling, gradient descent might oscillate back and forth many times before converging to the optimal value. By scaling features such that both $ x_1 $ and $ x_2 $ range from 0 to 1, the contour plot becomes more circular, allowing gradient descent to find a direct path to the center.\n",
        "\n",
        "#### Methods of Feature Scaling\n",
        "\n",
        "1. **Min-Max Scaling**:\n",
        "   - For $ x_1 $ ranging from 300 to 2000, divide each $ x_1 $ by 2000 (max value):\n",
        "   $$ x_{1,\\text{scaled}} = \\frac{x_1}{2000} $$\n",
        "     This scales $x_1$ to range from 0.15 to 1.\n",
        "\n",
        "   - For $ x_2 $ ranging from 0 to 5, divide each $ x_2 $ by 5 (max value): $$ x_{2,\\text{scaled}} = \\frac{x_2}{5}$$\n",
        "     This scales $ x_2 $ to range from 0 to 1.\n",
        "\n",
        "2. **Mean Normalization**:\n",
        "   - Centering the values of $ x_1 $ and $ x_2 $ around 0:\n",
        "     $$  x_{1,\\text{normalized}} = \\frac{x_1 - \\mu_1}{\\text{max}(x_1) - \\text{min}(x_1)  $$\n",
        "     Where \\( \\mu_1 \\) is the mean of \\( x_1 \\).\n",
        "\n",
        "3. **Z-Score Normalization**:\n",
        "   - Standardizing the values of $ x_1 $ and $ x_2 $:\n",
        "     $$ x_{1,\\text{standardized}} = \\frac{x_1 - \\mu_1}{\\sigma_1}$$\n",
        "     Where \\( \\sigma_1 \\) is the standard deviation of \\( x_1 \\).\n",
        "\n",
        "#### Acceptable Ranges for Scaled Features\n",
        "\n",
        "Aim for scaled feature values to be within:\n",
        "\n",
        "$$ -1 \\leq x_j \\leq 1$$\n",
        "\n",
        "or at least within:\n",
        "\n",
        "$$ -3 \\leq x_j \\leq 3$$\n",
        "\n",
        "Extreme ranges such as $ -100 \\leq x_j \\leq 100 $ are too large and need rescaling, while very small ranges like $ -0.001 \\leq x_j \\leq 0.001 $ are also inappropriate. Proper feature scaling ensures that gradient descent converges efficiently without the risk of bouncing back and forth.\n",
        "\n",
        "There is no harm in feature scaling and it is generally beneficial for optimizing machine learning algorithms.\n"
      ],
      "metadata": {
        "id": "58VbYwdRfJNz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Checking Gradient Descent for Convergence\n",
        "\n",
        "The goal of gradient descent is to minimize the cost function $ J(\\mathbf{w}, b) $. To ensure that gradient descent is working properly, we can plot the cost function $ J $ at intervals of each iteration. This plot is known as a learning curve.\n",
        "\n",
        "#### Learning Curve\n",
        "\n",
        "A learning curve is a graph that shows the cost function $ J $ on the y-axis and the number of iterations on the x-axis. By observing the learning curve, we can determine whether the gradient descent algorithm is converging.\n",
        "\n",
        "- **Reducing Cost Function**: If the cost function $ J $ is decreasing over iterations, it indicates that the gradient descent is working correctly and the parameters are being optimized.\n",
        "- **Flattening Curve**: When the learning curve flattens out, it means that the cost function $ J $ has reached a minimum or is no longer decreasing significantly. This indicates that the algorithm has converged.\n"
      ],
      "metadata": {
        "id": "MR04eR5KqJHT"
      }
    }
  ]
}