{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V28"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "# import cupy as np\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "E0F7_l7t26sb"
      },
      "execution_count": 187,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('drive/MyDrive/Colab Notebooks/datasets/training_data.csv')\n",
        "unique_values = df.iloc[:, 0].unique()\n",
        "unique_values.sort()\n",
        "unique_values"
      ],
      "metadata": {
        "id": "fpONDRz065_v",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a435cc36-b510-4dd2-dad3-397d2ccbb2aa"
      },
      "execution_count": 188,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 0,  1,  2,  3,  4,  5,  6,  7,  8, 10, 11, 12, 13, 14, 15, 16, 17,\n",
              "       18, 19, 20, 21, 22, 23, 24])"
            ]
          },
          "metadata": {},
          "execution_count": 188
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pixel_intensities_1d = df.iloc[1:2,1:].to_numpy()\n",
        "\n",
        "# Reshape the 1D array to a 2D array (28x28)\n",
        "pixel_intensities_2d = pixel_intensities_1d.reshape(28, 28)\n",
        "\n",
        "# Display the grayscale image\n",
        "plt.imshow(pixel_intensities_2d, cmap='gray', interpolation='none')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 447
        },
        "id": "Q9d-hu8971iQ",
        "outputId": "72f8e8aa-ab3e-4302-ad06-36885f7565f8"
      },
      "execution_count": 189,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7b7a208963b0>"
            ]
          },
          "metadata": {},
          "execution_count": 189
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = df.to_numpy()\n",
        "# Define batch size\n",
        "batch_size = 500\n",
        "# Calculate number of batches\n",
        "num_batches = dataset.shape[0] // batch_size\n",
        "# Ensure the number of batches is as expected\n",
        "# Split into batches and store in a list\n",
        "batches = [dataset[i * batch_size:(i + 1) * batch_size] for i in range(num_batches)]"
      ],
      "metadata": {
        "id": "oisw6Hu69S_-"
      },
      "execution_count": 190,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize lists for X_batches and Y_batches\n",
        "X_batches = []\n",
        "Y_batches = []\n",
        "\n",
        "# Split each batch into X and Y\n",
        "for batch in batches:\n",
        "    X = batch[:, 1:]  # Everything except the first column\n",
        "    Y = batch[:, :1]  # The first column only\n",
        "    X_batches.append(X)\n",
        "    Y_batches.append(Y)"
      ],
      "metadata": {
        "id": "aNjLVO25-t7n"
      },
      "execution_count": 191,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Y_batches_ohe = []\n",
        "for batch in Y_batches:\n",
        "    arr = batch.flatten()  # Convert to 1D array\n",
        "\n",
        "    # Ensure all indices are within the valid range\n",
        "    if np.any(arr >= 25):\n",
        "        raise ValueError(f\"Some indices in `arr` are out of bounds for the identity matrix of size {25}.\")\n",
        "\n",
        "    # One-hot encode the array\n",
        "    enc_array = np.eye(25)[arr]\n",
        "\n",
        "    # Append to the list\n",
        "    Y_batches_ohe.append(enc_array)"
      ],
      "metadata": {
        "id": "oB7fY8pBCISb"
      },
      "execution_count": 192,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def normalize_columns(matrix):\n",
        "    # Convert matrix to numpy array if it's not already\n",
        "    matrix = np.array(matrix)\n",
        "\n",
        "    # Calculate min and max for each column\n",
        "    max_values = matrix.max(axis=0)\n",
        "\n",
        "    # Apply the normalization formula\n",
        "    normalized_matrix = matrix / max_values\n",
        "\n",
        "    return normalized_matrix\n",
        "\n",
        "X_normalized = []\n",
        "for i in X_batches:\n",
        "  X_normalized.append(normalize_columns(i))"
      ],
      "metadata": {
        "id": "Uu-O_qsaxdW2"
      },
      "execution_count": 193,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# class Layer:\n",
        "\n",
        "#   def __init__(self,dimension):\n",
        "#     self.w = np.ones(dimension)*0.1\n",
        "#     self.b = np.zeros(dimension[1])\n",
        "\n",
        "#   def forward(self, in_matrix):\n",
        "#     return np.tanh(np.dot(in_matrix, self.w) + self.b)\n",
        "\n",
        "#   def get_softmax(self, in_matrix):\n",
        "#     z = np.dot(in_matrix, self.w)\n",
        "#     exp_matrix = np.exp(z - np.max(z, axis=1, keepdims=True))  # for numerical stability\n",
        "#     sum_matrix = np.sum(exp_matrix, axis=1, keepdims=True)\n",
        "#     softmax = exp_matrix / sum_matrix\n",
        "#     return softmax"
      ],
      "metadata": {
        "id": "vfnrByypDlP2"
      },
      "execution_count": 194,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# class NeuralNetwork:\n",
        "\n",
        "#   def __init__(self, no_features, architecture):\n",
        "\n",
        "#     self.layers = []\n",
        "\n",
        "#     for i,size in enumerate(architecture):\n",
        "#       #if the first layer\n",
        "#       if i == 0:\n",
        "#         layer = Layer((no_features, size))\n",
        "#         self.layers.append(layer)\n",
        "#         continue\n",
        "#       #if any other layer\n",
        "#       else:\n",
        "#         layer = Layer((architecture[i-1],size))\n",
        "#         self.layers.append(layer)\n",
        "#         continue\n",
        "\n",
        "#   def predict(self, X, target):\n",
        "\n",
        "#     predictions = {}\n",
        "\n",
        "#     self.z_values = [np.array(X)]\n",
        "\n",
        "#     for i,layer in enumerate(self.layers):\n",
        "\n",
        "#       # since i starts from 0 and there is already a value at z_values[0],\n",
        "#       # for i = 0, we can use z_values[0] only, we dont need to do z_values[i-1] after z is appended now there are 2 indices at z_values = [0], [1]\n",
        "#       # for i = 1, we can use z_values[1] as the previous value, so this keeps working\n",
        "\n",
        "#       # If it is the last layer then we are giving the last recorder z value for calculation of softmax\n",
        "#       if i == len(self.layers)-1:\n",
        "#         z = layer.get_softmax(self.z_values[i])\n",
        "#         self.z_values.append(z)\n",
        "#         continue\n",
        "\n",
        "#       # If it is not the last layer then we are giving the last recorded z value as the input for dot product\n",
        "#       else:\n",
        "#         z = layer.forward(self.z_values[i])\n",
        "#         self.z_values.append(z)\n",
        "#         continue\n",
        "\n",
        "#     #The last value in z_values is the predictions\n",
        "#     #In the case of a 3 layered neural network with 2 hidden layers we have z_values[0] as X and z_values[1] as z1 and z_values[2] as z2  and z_value[3] as predictions\n",
        "#     predictions['predictions'] = self.z_values[-1] #Last value of z_values hence it is predictionsI\n",
        "\n",
        "#     #calculating loss\n",
        "#     loss_matrix = np.sum(-target*np.log(predictions['predictions'] + 1e-10), axis=1, keepdims=True)\n",
        "#     predictions['loss'] = np.mean(loss_matrix) #scalar loss\n",
        "\n",
        "#     #classification rate\n",
        "#     sum = 0\n",
        "#     for i,j in zip(target, predictions['predictions']):\n",
        "#       if np.argmax(i) == np.argmax(j):\n",
        "#         sum += 1\n",
        "#     predictions['c_rate'] = sum / target.shape[0]\n",
        "\n",
        "#     return predictions\n",
        "\n",
        "\n",
        "#   def train(self, X, target, learning_rate, epochs):\n",
        "#     for i in range(epochs):\n",
        "\n",
        "#       prediction = self.predict(X, target)\n",
        "#       if i % 100 == 0:\n",
        "#         print(f\"loss = {prediction['loss']} classification_rate = {prediction['c_rate']}\")\n",
        "\n",
        "#       delta3 = prediction['predictions'] - target\n",
        "#       derivative_w3 = np.dot(self.z_values[2].T, delta3) / X.shape[0]\n",
        "#       derivative_b3 = np.sum(delta3, axis=0) / X.shape[0]\n",
        "\n",
        "#       delta2 = np.dot(delta3 ,self.layers[2].w.T) * (1 - np.tanh(self.z_values[2])**2)\n",
        "#       derivative_w2 = np.dot(self.z_values[1].T, delta2) / X.shape[0]\n",
        "#       derivative_b2 = np.sum(delta2, axis=0) / X.shape[0]\n",
        "\n",
        "#       delta1 = np.dot(delta2, self.layers[1].w.T) * (1 - (np.tanh(self.z_values[1])**2))\n",
        "#       derivative_w1 = np.dot(self.z_values[0].T, delta1) / X.shape[0]\n",
        "#       derivative_b1 = np.sum(delta1, axis=0) / X.shape[0]\n",
        "\n",
        "#       self.layers[0].w -= learning_rate*derivative_w1\n",
        "#       self.layers[0].b -= learning_rate*derivative_b1\n",
        "\n",
        "#       self.layers[1].w -= learning_rate*derivative_w2\n",
        "#       self.layers[1].b -= learning_rate*derivative_b2\n",
        "\n",
        "#       self.layers[2].w -= learning_rate*derivative_w3\n",
        "#       self.layers[2].b -= learning_rate*derivative_b3\n"
      ],
      "metadata": {
        "id": "-HTc10CDOjXL"
      },
      "execution_count": 195,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Layer:\n",
        "\n",
        "  # W is a matrix of size D*N where D = no. of nodes and N = no. of input\n",
        "  # B is the bias of the layer which is a matrix of D*1 = where D = no. of nodes\n",
        "  def __init__(self,w,b):\n",
        "    self.w = w\n",
        "    self.b = b\n",
        "\n",
        "  def sigmoid(self,x):\n",
        "    return 1/(1+np.exp(-x))\n",
        "\n",
        "  # Given an input layer it gives output after multiplying and applying tanh activation function\n",
        "  def get_output(self,in_matrix):\n",
        "    output = np.dot(in_matrix,self.w) + self.b\n",
        "    output = self.sigmoid(output)\n",
        "    return output\n",
        "\n",
        "  def get_softmax(self, in_matrix):\n",
        "    get_softm_of = np.dot(in_matrix,self.w) + self.b\n",
        "    # Subtract the max value in each row for numerical stability\n",
        "    exp_output = np.exp(get_softm_of)\n",
        "    softmax = exp_output / np.sum(exp_output, axis = 1, keepdims = True)\n",
        "    return softmax\n",
        "\n",
        "\n",
        "def l_matrix(target, prediction):\n",
        "  loss_matrix = -np.sum(target * np.log(prediction), axis=1, keepdims=True)\n",
        "  return loss_matrix\n",
        "\n",
        "def loss_scalar(target, prediction):\n",
        "  loss_matrix = l_matrix(target, prediction)\n",
        "  loss = np.mean(loss_matrix)\n",
        "  return loss\n",
        "\n",
        "def classification_rate(target, prediction):\n",
        "  sum = 0\n",
        "  for i, j in zip(target, prediction):\n",
        "    if np.argmax(i) == np.argmax(j):\n",
        "      sum += 1\n",
        "  c_rate = sum / target.shape[0]\n",
        "  return c_rate\n",
        "\n",
        "\n",
        "def derivative_w3(z_2, target, predictions):\n",
        "  return np.dot(z_2.T, (predictions - target)) / z_2.shape[0]\n",
        "\n",
        "def derivative_b3(target, predictions):\n",
        "  return np.sum(predictions - target, axis=0) / target.shape[0]\n",
        "\n",
        "def derivative_w2(z_1, z_2, target, predictions, w3):\n",
        "  dZ = np.dot((predictions - target), w3.T) * z_2 * (1 - (z_2))\n",
        "  return np.dot(z_1.T, dZ) / z_1.shape[0]\n",
        "\n",
        "def derivative_b2(z_2, target, predictions, w3):\n",
        "  dZ = np.dot((predictions - target), w3.T) * z_2 * (1 - (z_2))\n",
        "  return np.sum(dZ, axis=0) / z_2.shape[0]\n",
        "\n",
        "def derivative_w1(z_0, target, predictions, w3, z_2, w2, z_1):\n",
        "  dZ = np.dot((predictions - target), w3.T) * z_2 * (1 - (z_2))\n",
        "  dZ1 = np.dot(dZ, w2.T) * z_1 * (1 - (z_1))\n",
        "  return np.dot(z_0.T, dZ1) / z_0.shape[0]\n",
        "\n",
        "def derivative_b1(target, predictions, w3, z_2, w2, z_1):\n",
        "  dZ = np.dot((predictions - target), w3.T) * z_2* (1 - (z_2))\n",
        "  dZ1 = np.dot(dZ, w2.T) * z_1 * (1 - (z_1))\n",
        "  return np.sum(dZ1, axis=0) / z_1.shape[0]\n",
        "\n",
        "def gradient_step(learning_rate,w1,b1,w2,b2,w3,b3,target,predictions,z_1,z_2,z_0):\n",
        "  w1 -= learning_rate*(derivative_w1(z_0, target, predictions, w3, z_2, w2, z_1))\n",
        "  b1 -= learning_rate*(derivative_b1(target, predictions, w3, z_2, w2, z_1))\n",
        "\n",
        "  w2 -= learning_rate*(derivative_w2(z_1, z_2, target, predictions, w3))\n",
        "  b2 -= learning_rate*(derivative_b2(z_2, target, predictions, w3))\n",
        "\n",
        "  w3 -= learning_rate*(derivative_w3(z_2, target, predictions))\n",
        "  b3 -= learning_rate*(derivative_b3(target, predictions))\n",
        "\n",
        "  return w1, b1, w2, b2, w3, b3\n",
        ""
      ],
      "metadata": {
        "id": "Um9Fo1jYFBPH"
      },
      "execution_count": 196,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "z_0 = X_normalized[0]\n",
        "w1 = np.random.randn(784, 20)\n",
        "b1 = np.random.randn(20)\n",
        "l1 = Layer(w1, b1)\n",
        "z_1 = l1.get_output(z_0)\n",
        "\n",
        "w2 = np.random.randn(20, 35)\n",
        "b2 = np.random.randn(35)\n",
        "l2 = Layer(w2, b2)\n",
        "z_2 = l2.get_output(z_1)\n",
        "\n",
        "w3 = np.random.randn(35, 25)\n",
        "b3 = np.random.randn(25)\n",
        "l3 = Layer(w3, b3)\n",
        "z_3 = l3.get_softmax(z_2)\n",
        "\n",
        "print(f'Loss = {loss_scalar(Y_batches_ohe[0], z_3)}\\nAccuracy = {classification_rate(Y_batches_ohe[0], z_3)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pZLli_WnFO79",
        "outputId": "62cc3d3b-eb22-4a15-c8a3-1418a7b1aab0"
      },
      "execution_count": 198,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss = 5.915496386009589\n",
            "Accuracy = 0.048\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for j in range(len(X_normalized[0])-1):\n",
        "  print(f'Training for {j} - batch')\n",
        "  z_0 = X_normalized[j]\n",
        "  for i in range(1100):\n",
        "    l1.w, l1.b, l2.w, l2.b, l3.w, l3.b = gradient_step(0.1,l1.w,l1.b,l2.w,l2.b,l3.w,l3.b,Y_batches_ohe[j],z_3,z_1,z_2,z_0)\n",
        "    z_1 = l1.get_output(z_0)\n",
        "    z_2 = l2.get_output(z_1)\n",
        "    z_3 = l3.get_softmax(z_2)\n",
        "\n",
        "    if (i % 100 == 0):\n",
        "      print(f'{i} - iteration')\n",
        "      print(f'loss = {loss_scalar(Y_batches_ohe[j], z_3)}')\n",
        "      print(f'accuracy = {classification_rate(Y_batches_ohe[j], z_3)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "dlb2uKN3ZC6z",
        "outputId": "306d1366-74e9-4d5b-a0ca-4b70cdba99b2"
      },
      "execution_count": 199,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training for 0 - batch\n",
            "0 - iteration\n",
            "loss = 5.6587549124205525\n",
            "accuracy = 0.048\n",
            "100 - iteration\n",
            "loss = 3.014557157760313\n",
            "accuracy = 0.122\n",
            "200 - iteration\n",
            "loss = 2.8956398102844543\n",
            "accuracy = 0.166\n",
            "300 - iteration\n",
            "loss = 2.7953440125238753\n",
            "accuracy = 0.192\n",
            "400 - iteration\n",
            "loss = 2.7069495876158327\n",
            "accuracy = 0.228\n",
            "500 - iteration\n",
            "loss = 2.6290110612399165\n",
            "accuracy = 0.256\n",
            "600 - iteration\n",
            "loss = 2.5617271674063784\n",
            "accuracy = 0.27\n",
            "700 - iteration\n",
            "loss = 2.5042143564567834\n",
            "accuracy = 0.292\n",
            "800 - iteration\n",
            "loss = 2.4533067897672898\n",
            "accuracy = 0.298\n",
            "900 - iteration\n",
            "loss = 2.406339283579654\n",
            "accuracy = 0.314\n",
            "1000 - iteration\n",
            "loss = 2.361789758900205\n",
            "accuracy = 0.332\n",
            "Training for 1 - batch\n",
            "0 - iteration\n",
            "loss = 2.6341894642601185\n",
            "accuracy = 0.248\n",
            "100 - iteration\n",
            "loss = 2.4963249283072946\n",
            "accuracy = 0.26\n",
            "200 - iteration\n",
            "loss = 2.3897263906980544\n",
            "accuracy = 0.28\n",
            "300 - iteration\n",
            "loss = 2.293572502819852\n",
            "accuracy = 0.316\n",
            "400 - iteration\n",
            "loss = 2.163092298441156\n",
            "accuracy = 0.354\n",
            "500 - iteration\n",
            "loss = 2.062349665865212\n",
            "accuracy = 0.374\n",
            "600 - iteration\n",
            "loss = 1.9725893977016284\n",
            "accuracy = 0.41\n",
            "700 - iteration\n",
            "loss = 1.8917945685141848\n",
            "accuracy = 0.436\n",
            "800 - iteration\n",
            "loss = 1.8185148074327409\n",
            "accuracy = 0.456\n",
            "900 - iteration\n",
            "loss = 1.751247108951498\n",
            "accuracy = 0.466\n",
            "1000 - iteration\n",
            "loss = 1.6892282755634287\n",
            "accuracy = 0.488\n",
            "Training for 2 - batch\n",
            "0 - iteration\n",
            "loss = 1.946241878906392\n",
            "accuracy = 0.41\n",
            "100 - iteration\n",
            "loss = 1.7444926007750532\n",
            "accuracy = 0.5\n",
            "200 - iteration\n",
            "loss = 1.6561548336120646\n",
            "accuracy = 0.528\n",
            "300 - iteration\n",
            "loss = 1.5831062514581573\n",
            "accuracy = 0.536\n",
            "400 - iteration\n",
            "loss = 1.5186589135610689\n",
            "accuracy = 0.57\n",
            "500 - iteration\n",
            "loss = 1.460565710005577\n",
            "accuracy = 0.582\n",
            "600 - iteration\n",
            "loss = 1.407060970875627\n",
            "accuracy = 0.596\n",
            "700 - iteration\n",
            "loss = 1.3885770296435103\n",
            "accuracy = 0.616\n",
            "800 - iteration\n",
            "loss = 1.324734133018029\n",
            "accuracy = 0.64\n",
            "900 - iteration\n",
            "loss = 1.2730759215721303\n",
            "accuracy = 0.658\n",
            "1000 - iteration\n",
            "loss = 1.2232121967821963\n",
            "accuracy = 0.672\n",
            "Training for 3 - batch\n",
            "0 - iteration\n",
            "loss = 1.544828453553415\n",
            "accuracy = 0.538\n",
            "100 - iteration\n",
            "loss = 1.3794527334780593\n",
            "accuracy = 0.574\n",
            "200 - iteration\n",
            "loss = 1.3176929596171096\n",
            "accuracy = 0.614\n",
            "300 - iteration\n",
            "loss = 1.265060718209716\n",
            "accuracy = 0.624\n",
            "400 - iteration\n",
            "loss = 1.2194415571350519\n",
            "accuracy = 0.64\n",
            "500 - iteration\n",
            "loss = 1.17927891404339\n",
            "accuracy = 0.66\n",
            "600 - iteration\n",
            "loss = 1.1446682478714896\n",
            "accuracy = 0.678\n",
            "700 - iteration\n",
            "loss = 1.114554987644728\n",
            "accuracy = 0.694\n",
            "800 - iteration\n",
            "loss = 1.0855191074099955\n",
            "accuracy = 0.712\n",
            "900 - iteration\n",
            "loss = 1.0566126465239665\n",
            "accuracy = 0.72\n",
            "1000 - iteration\n",
            "loss = 1.0280467262769135\n",
            "accuracy = 0.726\n",
            "Training for 4 - batch\n",
            "0 - iteration\n",
            "loss = 1.2412660615017603\n",
            "accuracy = 0.622\n",
            "100 - iteration\n",
            "loss = 1.073563499814307\n",
            "accuracy = 0.684\n",
            "200 - iteration\n",
            "loss = 1.02136064398835\n",
            "accuracy = 0.698\n",
            "300 - iteration\n",
            "loss = 0.9791036768289191\n",
            "accuracy = 0.714\n",
            "400 - iteration\n",
            "loss = 0.943196772424943\n",
            "accuracy = 0.728\n",
            "500 - iteration\n",
            "loss = 0.9108260019084985\n",
            "accuracy = 0.744\n",
            "600 - iteration\n",
            "loss = 0.8802068632668109\n",
            "accuracy = 0.768\n",
            "700 - iteration\n",
            "loss = 0.8503882136335124\n",
            "accuracy = 0.782\n",
            "800 - iteration\n",
            "loss = 0.8213198579053124\n",
            "accuracy = 0.784\n",
            "900 - iteration\n",
            "loss = 0.7942614677731182\n",
            "accuracy = 0.788\n",
            "1000 - iteration\n",
            "loss = 0.773939202579656\n",
            "accuracy = 0.788\n",
            "Training for 5 - batch\n",
            "0 - iteration\n",
            "loss = 1.1004395136735319\n",
            "accuracy = 0.644\n",
            "100 - iteration\n",
            "loss = 0.8900613082595236\n",
            "accuracy = 0.724\n",
            "200 - iteration\n",
            "loss = 0.840581535168217\n",
            "accuracy = 0.748\n",
            "300 - iteration\n",
            "loss = 0.8027562834986731\n",
            "accuracy = 0.762\n",
            "400 - iteration\n",
            "loss = 0.7706516029224026\n",
            "accuracy = 0.782\n",
            "500 - iteration\n",
            "loss = 0.7414634440128312\n",
            "accuracy = 0.788\n",
            "600 - iteration\n",
            "loss = 0.71405614752462\n",
            "accuracy = 0.796\n",
            "700 - iteration\n",
            "loss = 0.6883584102015818\n",
            "accuracy = 0.802\n",
            "800 - iteration\n",
            "loss = 0.6643146688539331\n",
            "accuracy = 0.81\n",
            "900 - iteration\n",
            "loss = 0.6416170227474045\n",
            "accuracy = 0.822\n",
            "1000 - iteration\n",
            "loss = 0.6200222689950845\n",
            "accuracy = 0.83\n",
            "Training for 6 - batch\n",
            "0 - iteration\n",
            "loss = 1.0675258027712677\n",
            "accuracy = 0.628\n",
            "100 - iteration\n",
            "loss = 0.9332062817566643\n",
            "accuracy = 0.688\n",
            "200 - iteration\n",
            "loss = 0.744721045800511\n",
            "accuracy = 0.8\n",
            "300 - iteration\n",
            "loss = 0.7098706667313223\n",
            "accuracy = 0.82\n",
            "400 - iteration\n",
            "loss = 0.6910831642103985\n",
            "accuracy = 0.826\n",
            "500 - iteration\n",
            "loss = 0.6603102486344807\n",
            "accuracy = 0.836\n",
            "600 - iteration\n",
            "loss = 0.6303160486613395\n",
            "accuracy = 0.85\n",
            "700 - iteration\n",
            "loss = 0.6248718838190861\n",
            "accuracy = 0.832\n",
            "800 - iteration\n",
            "loss = 0.6226718385436911\n",
            "accuracy = 0.83\n",
            "900 - iteration\n",
            "loss = 0.5693704352023188\n",
            "accuracy = 0.868\n",
            "1000 - iteration\n",
            "loss = 0.5561391281143726\n",
            "accuracy = 0.864\n",
            "Training for 7 - batch\n",
            "0 - iteration\n",
            "loss = 1.3076374565907731\n",
            "accuracy = 0.568\n",
            "100 - iteration\n",
            "loss = 0.5725612545821612\n",
            "accuracy = 0.87\n",
            "200 - iteration\n",
            "loss = 0.5337162174692732\n",
            "accuracy = 0.884\n",
            "300 - iteration\n",
            "loss = 0.5060153885875693\n",
            "accuracy = 0.89\n",
            "400 - iteration\n",
            "loss = 0.483730184698089\n",
            "accuracy = 0.908\n",
            "500 - iteration\n",
            "loss = 0.4645376857472962\n",
            "accuracy = 0.916\n",
            "600 - iteration\n",
            "loss = 0.447260031679369\n",
            "accuracy = 0.918\n",
            "700 - iteration\n",
            "loss = 0.4313989681940841\n",
            "accuracy = 0.922\n",
            "800 - iteration\n",
            "loss = 0.4168171288792919\n",
            "accuracy = 0.93\n",
            "900 - iteration\n",
            "loss = 0.40337737896650844\n",
            "accuracy = 0.936\n",
            "1000 - iteration\n",
            "loss = 0.3908615566299502\n",
            "accuracy = 0.94\n",
            "Training for 8 - batch\n",
            "0 - iteration\n",
            "loss = 0.6656785960940781\n",
            "accuracy = 0.826\n",
            "100 - iteration\n",
            "loss = 0.4733797492798473\n",
            "accuracy = 0.884\n",
            "200 - iteration\n",
            "loss = 0.45550099647594344\n",
            "accuracy = 0.894\n",
            "300 - iteration\n",
            "loss = 0.4174786505320018\n",
            "accuracy = 0.908\n",
            "400 - iteration\n",
            "loss = 0.3975088466883767\n",
            "accuracy = 0.92\n",
            "500 - iteration\n",
            "loss = 0.3800024183575199\n",
            "accuracy = 0.928\n",
            "600 - iteration\n",
            "loss = 0.364192740576057\n",
            "accuracy = 0.936\n",
            "700 - iteration\n",
            "loss = 0.3496818343527454\n",
            "accuracy = 0.942\n",
            "800 - iteration\n",
            "loss = 0.3362664439126899\n",
            "accuracy = 0.952\n",
            "900 - iteration\n",
            "loss = 0.32386232879353216\n",
            "accuracy = 0.954\n",
            "1000 - iteration\n",
            "loss = 0.31239405965408945\n",
            "accuracy = 0.96\n",
            "Training for 9 - batch\n",
            "0 - iteration\n",
            "loss = 0.6210501805762049\n",
            "accuracy = 0.812\n",
            "100 - iteration\n",
            "loss = 0.37562540054796156\n",
            "accuracy = 0.914\n",
            "200 - iteration\n",
            "loss = 0.33522755103346963\n",
            "accuracy = 0.938\n",
            "300 - iteration\n",
            "loss = 0.31651267134934213\n",
            "accuracy = 0.944\n",
            "400 - iteration\n",
            "loss = 0.3024782658216536\n",
            "accuracy = 0.948\n",
            "500 - iteration\n",
            "loss = 0.290875108684711\n",
            "accuracy = 0.95\n",
            "600 - iteration\n",
            "loss = 0.2807890536622634\n",
            "accuracy = 0.95\n",
            "700 - iteration\n",
            "loss = 0.2717487940324892\n",
            "accuracy = 0.95\n",
            "800 - iteration\n",
            "loss = 0.2634793008447971\n",
            "accuracy = 0.956\n",
            "900 - iteration\n",
            "loss = 0.2558059027374761\n",
            "accuracy = 0.96\n",
            "1000 - iteration\n",
            "loss = 0.248609991923484\n",
            "accuracy = 0.96\n",
            "Training for 10 - batch\n",
            "0 - iteration\n",
            "loss = 0.7477965244531389\n",
            "accuracy = 0.726\n",
            "100 - iteration\n",
            "loss = 0.3295042091964409\n",
            "accuracy = 0.916\n",
            "200 - iteration\n",
            "loss = 0.2906542996869244\n",
            "accuracy = 0.942\n",
            "300 - iteration\n",
            "loss = 0.274185141058628\n",
            "accuracy = 0.946\n",
            "400 - iteration\n",
            "loss = 0.2611909261421454\n",
            "accuracy = 0.962\n",
            "500 - iteration\n",
            "loss = 0.25028370208177986\n",
            "accuracy = 0.968\n",
            "600 - iteration\n",
            "loss = 0.240791979309312\n",
            "accuracy = 0.968\n",
            "700 - iteration\n",
            "loss = 0.2323321415245603\n",
            "accuracy = 0.974\n",
            "800 - iteration\n",
            "loss = 0.2246738545235062\n",
            "accuracy = 0.974\n",
            "900 - iteration\n",
            "loss = 0.2176652475307888\n",
            "accuracy = 0.976\n",
            "1000 - iteration\n",
            "loss = 0.21119750783720923\n",
            "accuracy = 0.976\n",
            "Training for 11 - batch\n",
            "0 - iteration\n",
            "loss = 0.8879682989013377\n",
            "accuracy = 0.71\n",
            "100 - iteration\n",
            "loss = 0.30906727260196615\n",
            "accuracy = 0.926\n",
            "200 - iteration\n",
            "loss = 0.2798541279318735\n",
            "accuracy = 0.936\n",
            "300 - iteration\n",
            "loss = 0.262718869575597\n",
            "accuracy = 0.952\n",
            "400 - iteration\n",
            "loss = 0.2496597441290424\n",
            "accuracy = 0.956\n",
            "500 - iteration\n",
            "loss = 0.23895853061773\n",
            "accuracy = 0.956\n",
            "600 - iteration\n",
            "loss = 0.22983123919478407\n",
            "accuracy = 0.958\n",
            "700 - iteration\n",
            "loss = 0.22183851233610302\n",
            "accuracy = 0.958\n",
            "800 - iteration\n",
            "loss = 0.21470608060104762\n",
            "accuracy = 0.958\n",
            "900 - iteration\n",
            "loss = 0.2082510399147833\n",
            "accuracy = 0.956\n",
            "1000 - iteration\n",
            "loss = 0.20234522503811098\n",
            "accuracy = 0.956\n",
            "Training for 12 - batch\n",
            "0 - iteration\n",
            "loss = 0.6184854854146121\n",
            "accuracy = 0.8\n",
            "100 - iteration\n",
            "loss = 0.2467662489183487\n",
            "accuracy = 0.958\n",
            "200 - iteration\n",
            "loss = 0.22563387821320474\n",
            "accuracy = 0.966\n",
            "300 - iteration\n",
            "loss = 0.21215056790984055\n",
            "accuracy = 0.968\n",
            "400 - iteration\n",
            "loss = 0.20155512575562165\n",
            "accuracy = 0.968\n",
            "500 - iteration\n",
            "loss = 0.1926994527970853\n",
            "accuracy = 0.972\n",
            "600 - iteration\n",
            "loss = 0.18504765422187539\n",
            "accuracy = 0.972\n",
            "700 - iteration\n",
            "loss = 0.1782575175237203\n",
            "accuracy = 0.972\n",
            "800 - iteration\n",
            "loss = 0.17209263716994086\n",
            "accuracy = 0.972\n",
            "900 - iteration\n",
            "loss = 0.16638038460766483\n",
            "accuracy = 0.974\n",
            "1000 - iteration\n",
            "loss = 0.16098211927486927\n",
            "accuracy = 0.974\n",
            "Training for 13 - batch\n",
            "0 - iteration\n",
            "loss = 0.6983252801148127\n",
            "accuracy = 0.778\n",
            "100 - iteration\n",
            "loss = 0.19398763875419156\n",
            "accuracy = 0.952\n",
            "200 - iteration\n",
            "loss = 0.18104701596048792\n",
            "accuracy = 0.96\n",
            "300 - iteration\n",
            "loss = 0.17267170889686476\n",
            "accuracy = 0.966\n",
            "400 - iteration\n",
            "loss = 0.16617756508180748\n",
            "accuracy = 0.972\n",
            "500 - iteration\n",
            "loss = 0.16071192953172694\n",
            "accuracy = 0.98\n",
            "600 - iteration\n",
            "loss = 0.15587667299695562\n",
            "accuracy = 0.984\n",
            "700 - iteration\n",
            "loss = 0.15145555104126107\n",
            "accuracy = 0.984\n",
            "800 - iteration\n",
            "loss = 0.1473207176167845\n",
            "accuracy = 0.986\n",
            "900 - iteration\n",
            "loss = 0.14339542704880365\n",
            "accuracy = 0.986\n",
            "1000 - iteration\n",
            "loss = 0.13963904308932196\n",
            "accuracy = 0.988\n",
            "Training for 14 - batch\n",
            "0 - iteration\n",
            "loss = 2.5298411395157028\n",
            "accuracy = 0.49\n",
            "100 - iteration\n",
            "loss = 1.677341612950112\n",
            "accuracy = 0.58\n",
            "200 - iteration\n",
            "loss = 1.4229580353838\n",
            "accuracy = 0.62\n",
            "300 - iteration\n",
            "loss = 1.2504473083464231\n",
            "accuracy = 0.662\n",
            "400 - iteration\n",
            "loss = 1.1722166772792757\n",
            "accuracy = 0.68\n",
            "500 - iteration\n",
            "loss = 1.1045827665081276\n",
            "accuracy = 0.678\n",
            "600 - iteration\n",
            "loss = 1.0917453623886573\n",
            "accuracy = 0.67\n",
            "700 - iteration\n",
            "loss = 0.9877503365387912\n",
            "accuracy = 0.698\n",
            "800 - iteration\n",
            "loss = 0.8933572929408438\n",
            "accuracy = 0.736\n",
            "900 - iteration\n",
            "loss = 0.8055325767143342\n",
            "accuracy = 0.772\n",
            "1000 - iteration\n",
            "loss = 0.7690529021596608\n",
            "accuracy = 0.786\n",
            "Training for 15 - batch\n",
            "0 - iteration\n",
            "loss = 2.698918922367492\n",
            "accuracy = 0.368\n",
            "100 - iteration\n",
            "loss = 1.7915043421160597\n",
            "accuracy = 0.462\n",
            "200 - iteration\n",
            "loss = 1.5654288779240932\n",
            "accuracy = 0.508\n",
            "300 - iteration\n",
            "loss = 1.4250543512297449\n",
            "accuracy = 0.54\n",
            "400 - iteration\n",
            "loss = 1.3290146318363254\n",
            "accuracy = 0.57\n",
            "500 - iteration\n",
            "loss = 1.2556452778083258\n",
            "accuracy = 0.616\n",
            "600 - iteration\n",
            "loss = 1.1947335112258852\n",
            "accuracy = 0.648\n",
            "700 - iteration\n",
            "loss = 1.1439833665287447\n",
            "accuracy = 0.656\n",
            "800 - iteration\n",
            "loss = 1.10118345391184\n",
            "accuracy = 0.668\n",
            "900 - iteration\n",
            "loss = 1.062389060323997\n",
            "accuracy = 0.672\n",
            "1000 - iteration\n",
            "loss = 1.0261580242928523\n",
            "accuracy = 0.676\n",
            "Training for 16 - batch\n",
            "0 - iteration\n",
            "loss = 1.8920482610701796\n",
            "accuracy = 0.426\n",
            "100 - iteration\n",
            "loss = 1.569024261485291\n",
            "accuracy = 0.484\n",
            "200 - iteration\n",
            "loss = 1.4347320642389485\n",
            "accuracy = 0.534\n",
            "300 - iteration\n",
            "loss = 1.359983428125654\n",
            "accuracy = 0.544\n",
            "400 - iteration\n",
            "loss = 1.2888149681009395\n",
            "accuracy = 0.576\n",
            "500 - iteration\n",
            "loss = 1.2340848985076431\n",
            "accuracy = 0.584\n",
            "600 - iteration\n",
            "loss = 1.1891198349103171\n",
            "accuracy = 0.606\n",
            "700 - iteration\n",
            "loss = 1.1489964980157807\n",
            "accuracy = 0.62\n",
            "800 - iteration\n",
            "loss = 1.1126146856936336\n",
            "accuracy = 0.644\n",
            "900 - iteration\n",
            "loss = 1.0792972537301477\n",
            "accuracy = 0.644\n",
            "1000 - iteration\n",
            "loss = 1.0493191601562977\n",
            "accuracy = 0.656\n",
            "Training for 17 - batch\n",
            "0 - iteration\n",
            "loss = 1.781813077355166\n",
            "accuracy = 0.454\n",
            "100 - iteration\n",
            "loss = 1.2917110546551718\n",
            "accuracy = 0.596\n",
            "200 - iteration\n",
            "loss = 1.1841424689932858\n",
            "accuracy = 0.644\n",
            "300 - iteration\n",
            "loss = 1.1038266693317924\n",
            "accuracy = 0.664\n",
            "400 - iteration\n",
            "loss = 1.0448789185416558\n",
            "accuracy = 0.678\n",
            "500 - iteration\n",
            "loss = 0.9972317618205835\n",
            "accuracy = 0.688\n",
            "600 - iteration\n",
            "loss = 0.9556759945367485\n",
            "accuracy = 0.704\n",
            "700 - iteration\n",
            "loss = 0.9182570479092093\n",
            "accuracy = 0.72\n",
            "800 - iteration\n",
            "loss = 0.8841277892200935\n",
            "accuracy = 0.736\n",
            "900 - iteration\n",
            "loss = 0.8535078195540993\n",
            "accuracy = 0.746\n",
            "1000 - iteration\n",
            "loss = 0.8260185094375273\n",
            "accuracy = 0.756\n",
            "Training for 18 - batch\n",
            "0 - iteration\n",
            "loss = 1.7188378343377213\n",
            "accuracy = 0.472\n",
            "100 - iteration\n",
            "loss = 1.2685438668178293\n",
            "accuracy = 0.608\n",
            "200 - iteration\n",
            "loss = 1.150660645371055\n",
            "accuracy = 0.638\n",
            "300 - iteration\n",
            "loss = 1.0766269882532593\n",
            "accuracy = 0.666\n",
            "400 - iteration\n",
            "loss = 1.023637021516889\n",
            "accuracy = 0.682\n",
            "500 - iteration\n",
            "loss = 0.9806457575670956\n",
            "accuracy = 0.698\n",
            "600 - iteration\n",
            "loss = 0.9434304078792899\n",
            "accuracy = 0.702\n",
            "700 - iteration\n",
            "loss = 0.9109951702580821\n",
            "accuracy = 0.706\n",
            "800 - iteration\n",
            "loss = 0.8820317408918552\n",
            "accuracy = 0.714\n",
            "900 - iteration\n",
            "loss = 0.8547422150390768\n",
            "accuracy = 0.724\n",
            "1000 - iteration\n",
            "loss = 0.8282470694605291\n",
            "accuracy = 0.73\n",
            "Training for 19 - batch\n",
            "0 - iteration\n",
            "loss = 1.5519789355705427\n",
            "accuracy = 0.484\n",
            "100 - iteration\n",
            "loss = 1.1463834585170745\n",
            "accuracy = 0.616\n",
            "200 - iteration\n",
            "loss = 1.0451138891661924\n",
            "accuracy = 0.646\n",
            "300 - iteration\n",
            "loss = 0.9752507012199283\n",
            "accuracy = 0.664\n",
            "400 - iteration\n",
            "loss = 0.9359522516585119\n",
            "accuracy = 0.686\n",
            "500 - iteration\n",
            "loss = 0.8918406253061127\n",
            "accuracy = 0.69\n",
            "600 - iteration\n",
            "loss = 0.8536436543611824\n",
            "accuracy = 0.706\n",
            "700 - iteration\n",
            "loss = 0.8186966509131919\n",
            "accuracy = 0.724\n",
            "800 - iteration\n",
            "loss = 0.7873804170857928\n",
            "accuracy = 0.74\n",
            "900 - iteration\n",
            "loss = 0.7750563407882404\n",
            "accuracy = 0.744\n",
            "1000 - iteration\n",
            "loss = 0.7439008696402882\n",
            "accuracy = 0.756\n",
            "Training for 20 - batch\n",
            "0 - iteration\n",
            "loss = 1.4013763878943937\n",
            "accuracy = 0.554\n",
            "100 - iteration\n",
            "loss = 0.9763507954703152\n",
            "accuracy = 0.68\n",
            "200 - iteration\n",
            "loss = 0.8949426675285972\n",
            "accuracy = 0.722\n",
            "300 - iteration\n",
            "loss = 0.8400860876152642\n",
            "accuracy = 0.742\n",
            "400 - iteration\n",
            "loss = 0.7977475777509253\n",
            "accuracy = 0.754\n",
            "500 - iteration\n",
            "loss = 0.7614983334525989\n",
            "accuracy = 0.77\n",
            "600 - iteration\n",
            "loss = 0.727461606190165\n",
            "accuracy = 0.786\n",
            "700 - iteration\n",
            "loss = 0.6980551840173925\n",
            "accuracy = 0.798\n",
            "800 - iteration\n",
            "loss = 0.6721063395196258\n",
            "accuracy = 0.804\n",
            "900 - iteration\n",
            "loss = 0.6491250476798058\n",
            "accuracy = 0.816\n",
            "1000 - iteration\n",
            "loss = 0.6284322862657523\n",
            "accuracy = 0.836\n",
            "Training for 21 - batch\n",
            "0 - iteration\n",
            "loss = 1.360937484261358\n",
            "accuracy = 0.574\n",
            "100 - iteration\n",
            "loss = 0.9237060478537423\n",
            "accuracy = 0.706\n",
            "200 - iteration\n",
            "loss = 0.827856421080475\n",
            "accuracy = 0.744\n",
            "300 - iteration\n",
            "loss = 0.7649187994528681\n",
            "accuracy = 0.77\n",
            "400 - iteration\n",
            "loss = 0.7175038299550558\n",
            "accuracy = 0.782\n",
            "500 - iteration\n",
            "loss = 0.6796028042934037\n",
            "accuracy = 0.784\n",
            "600 - iteration\n",
            "loss = 0.6485164751276579\n",
            "accuracy = 0.804\n",
            "700 - iteration\n",
            "loss = 0.6221312567943236\n",
            "accuracy = 0.814\n",
            "800 - iteration\n",
            "loss = 0.5992722174985782\n",
            "accuracy = 0.834\n",
            "900 - iteration\n",
            "loss = 0.5788651784151985\n",
            "accuracy = 0.84\n",
            "1000 - iteration\n",
            "loss = 0.5602384181910084\n",
            "accuracy = 0.844\n",
            "Training for 22 - batch\n",
            "0 - iteration\n",
            "loss = 1.448295721037699\n",
            "accuracy = 0.532\n",
            "100 - iteration\n",
            "loss = 0.9434421902472029\n",
            "accuracy = 0.682\n",
            "200 - iteration\n",
            "loss = 0.828494512809139\n",
            "accuracy = 0.736\n",
            "300 - iteration\n",
            "loss = 0.7708608957228165\n",
            "accuracy = 0.766\n",
            "400 - iteration\n",
            "loss = 0.7296476948959647\n",
            "accuracy = 0.78\n",
            "500 - iteration\n",
            "loss = 0.6955353234381575\n",
            "accuracy = 0.794\n",
            "600 - iteration\n",
            "loss = 0.6688034259014862\n",
            "accuracy = 0.808\n",
            "700 - iteration\n",
            "loss = 0.6449502421422837\n",
            "accuracy = 0.814\n",
            "800 - iteration\n",
            "loss = 0.6228791366676785\n",
            "accuracy = 0.824\n",
            "900 - iteration\n",
            "loss = 0.6021020727241227\n",
            "accuracy = 0.83\n",
            "1000 - iteration\n",
            "loss = 0.5821871503501976\n",
            "accuracy = 0.836\n",
            "Training for 23 - batch\n",
            "0 - iteration\n",
            "loss = 1.3725324595577035\n",
            "accuracy = 0.588\n",
            "100 - iteration\n",
            "loss = 0.9866749963356655\n",
            "accuracy = 0.69\n",
            "200 - iteration\n",
            "loss = 0.9187152300996848\n",
            "accuracy = 0.708\n",
            "300 - iteration\n",
            "loss = 0.8273254175124594\n",
            "accuracy = 0.738\n",
            "400 - iteration\n",
            "loss = 0.7804665257162451\n",
            "accuracy = 0.764\n",
            "500 - iteration\n",
            "loss = 0.7376988732495837\n",
            "accuracy = 0.78\n",
            "600 - iteration\n",
            "loss = 0.6981690717419285\n",
            "accuracy = 0.8\n",
            "700 - iteration\n",
            "loss = 0.6651794571258213\n",
            "accuracy = 0.82\n",
            "800 - iteration\n",
            "loss = 0.6388780370615116\n",
            "accuracy = 0.83\n",
            "900 - iteration\n",
            "loss = 0.6166404535188121\n",
            "accuracy = 0.838\n",
            "1000 - iteration\n",
            "loss = 0.5969998658554466\n",
            "accuracy = 0.848\n",
            "Training for 24 - batch\n",
            "0 - iteration\n",
            "loss = 1.1317121969613724\n",
            "accuracy = 0.656\n",
            "100 - iteration\n",
            "loss = 0.7968336321032937\n",
            "accuracy = 0.754\n",
            "200 - iteration\n",
            "loss = 0.7309596306822845\n",
            "accuracy = 0.782\n",
            "300 - iteration\n",
            "loss = 0.6890161670628407\n",
            "accuracy = 0.802\n",
            "400 - iteration\n",
            "loss = 0.6570509328887845\n",
            "accuracy = 0.81\n",
            "500 - iteration\n",
            "loss = 0.6306882179260992\n",
            "accuracy = 0.834\n",
            "600 - iteration\n",
            "loss = 0.6080829630444722\n",
            "accuracy = 0.836\n",
            "700 - iteration\n",
            "loss = 0.5882190329601488\n",
            "accuracy = 0.852\n",
            "800 - iteration\n",
            "loss = 0.5704044654507793\n",
            "accuracy = 0.858\n",
            "900 - iteration\n",
            "loss = 0.554207891289538\n",
            "accuracy = 0.858\n",
            "1000 - iteration\n",
            "loss = 0.5393755909364042\n",
            "accuracy = 0.866\n",
            "Training for 25 - batch\n",
            "0 - iteration\n",
            "loss = 1.3767185692965886\n",
            "accuracy = 0.554\n",
            "100 - iteration\n",
            "loss = 0.7852465758496947\n",
            "accuracy = 0.76\n",
            "200 - iteration\n",
            "loss = 0.6745678064062062\n",
            "accuracy = 0.802\n",
            "300 - iteration\n",
            "loss = 0.6309574257231925\n",
            "accuracy = 0.816\n",
            "400 - iteration\n",
            "loss = 0.5774797556332281\n",
            "accuracy = 0.842\n",
            "500 - iteration\n",
            "loss = 0.545375426783428\n",
            "accuracy = 0.848\n",
            "600 - iteration\n",
            "loss = 0.5153146951422929\n",
            "accuracy = 0.868\n",
            "700 - iteration\n",
            "loss = 0.4877191018199459\n",
            "accuracy = 0.878\n",
            "800 - iteration\n",
            "loss = 0.46781968464009255\n",
            "accuracy = 0.884\n",
            "900 - iteration\n",
            "loss = 0.45017730921601584\n",
            "accuracy = 0.894\n",
            "1000 - iteration\n",
            "loss = 0.4342940712326707\n",
            "accuracy = 0.898\n",
            "Training for 26 - batch\n",
            "0 - iteration\n",
            "loss = 1.3049646806861441\n",
            "accuracy = 0.638\n",
            "100 - iteration\n",
            "loss = 0.7550632127782486\n",
            "accuracy = 0.768\n",
            "200 - iteration\n",
            "loss = 0.6744173848861924\n",
            "accuracy = 0.802\n",
            "300 - iteration\n",
            "loss = 0.6074181164596987\n",
            "accuracy = 0.84\n",
            "400 - iteration\n",
            "loss = 0.55599977481117\n",
            "accuracy = 0.868\n",
            "500 - iteration\n",
            "loss = 0.5287109528750439\n",
            "accuracy = 0.876\n",
            "600 - iteration\n",
            "loss = 0.5073929524358028\n",
            "accuracy = 0.884\n",
            "700 - iteration\n",
            "loss = 0.4891218726512185\n",
            "accuracy = 0.888\n",
            "800 - iteration\n",
            "loss = 0.47286941369229457\n",
            "accuracy = 0.9\n",
            "900 - iteration\n",
            "loss = 0.45807809505534713\n",
            "accuracy = 0.904\n",
            "1000 - iteration\n",
            "loss = 0.44440685375290356\n",
            "accuracy = 0.906\n",
            "Training for 27 - batch\n",
            "0 - iteration\n",
            "loss = 1.1165873192631643\n",
            "accuracy = 0.626\n",
            "100 - iteration\n",
            "loss = 0.7246869236961374\n",
            "accuracy = 0.774\n",
            "200 - iteration\n",
            "loss = 0.5903023828103878\n",
            "accuracy = 0.802\n",
            "300 - iteration\n",
            "loss = 0.5981690704416903\n",
            "accuracy = 0.81\n",
            "400 - iteration\n",
            "loss = 0.4939884188330483\n",
            "accuracy = 0.854\n",
            "500 - iteration\n",
            "loss = 0.46210015525718695\n",
            "accuracy = 0.866\n",
            "600 - iteration\n",
            "loss = 0.4324099166630905\n",
            "accuracy = 0.874\n",
            "700 - iteration\n",
            "loss = 0.7006848693016886\n",
            "accuracy = 0.776\n",
            "800 - iteration\n",
            "loss = 0.4009938029851538\n",
            "accuracy = 0.892\n",
            "900 - iteration\n",
            "loss = 0.38630386430906616\n",
            "accuracy = 0.9\n",
            "1000 - iteration\n",
            "loss = 0.37363228243558877\n",
            "accuracy = 0.9\n",
            "Training for 28 - batch\n",
            "0 - iteration\n",
            "loss = 1.396047711832317\n",
            "accuracy = 0.584\n",
            "100 - iteration\n",
            "loss = 0.6366714644729141\n",
            "accuracy = 0.776\n",
            "200 - iteration\n",
            "loss = 0.45787551380540675\n",
            "accuracy = 0.864\n",
            "300 - iteration\n",
            "loss = 0.4183646078154652\n",
            "accuracy = 0.88\n",
            "400 - iteration\n",
            "loss = 0.3900165833392887\n",
            "accuracy = 0.896\n",
            "500 - iteration\n",
            "loss = 0.3676571830541946\n",
            "accuracy = 0.904\n",
            "600 - iteration\n",
            "loss = 0.34896949321015386\n",
            "accuracy = 0.912\n",
            "700 - iteration\n",
            "loss = 0.3329157222696875\n",
            "accuracy = 0.91\n",
            "800 - iteration\n",
            "loss = 0.3189946641327453\n",
            "accuracy = 0.914\n",
            "900 - iteration\n",
            "loss = 0.30678953011412424\n",
            "accuracy = 0.916\n",
            "1000 - iteration\n",
            "loss = 0.2959177126110722\n",
            "accuracy = 0.918\n",
            "Training for 29 - batch\n",
            "0 - iteration\n",
            "loss = 1.1065991264586428\n",
            "accuracy = 0.65\n",
            "100 - iteration\n",
            "loss = 0.5996086251421923\n",
            "accuracy = 0.82\n",
            "200 - iteration\n",
            "loss = 0.5321532528825769\n",
            "accuracy = 0.838\n",
            "300 - iteration\n",
            "loss = 0.4845490962011388\n",
            "accuracy = 0.854\n",
            "400 - iteration\n",
            "loss = 0.449926499652688\n",
            "accuracy = 0.872\n",
            "500 - iteration\n",
            "loss = 0.42072293135666794\n",
            "accuracy = 0.894\n",
            "600 - iteration\n",
            "loss = 0.40003164915719164\n",
            "accuracy = 0.904\n",
            "700 - iteration\n",
            "loss = 0.38299067855607605\n",
            "accuracy = 0.902\n",
            "800 - iteration\n",
            "loss = 0.368042319760221\n",
            "accuracy = 0.902\n",
            "900 - iteration\n",
            "loss = 0.3546705050266534\n",
            "accuracy = 0.906\n",
            "1000 - iteration\n",
            "loss = 0.34254440318764207\n",
            "accuracy = 0.91\n",
            "Training for 30 - batch\n",
            "0 - iteration\n",
            "loss = 1.343487744589758\n",
            "accuracy = 0.572\n",
            "100 - iteration\n",
            "loss = 0.5674679917917075\n",
            "accuracy = 0.822\n",
            "200 - iteration\n",
            "loss = 0.4760505512664247\n",
            "accuracy = 0.87\n",
            "300 - iteration\n",
            "loss = 0.47173192659689245\n",
            "accuracy = 0.862\n",
            "400 - iteration\n",
            "loss = 0.413655244696331\n",
            "accuracy = 0.894\n",
            "500 - iteration\n",
            "loss = 0.39266721207015337\n",
            "accuracy = 0.904\n",
            "600 - iteration\n",
            "loss = 0.3754405563289641\n",
            "accuracy = 0.91\n",
            "700 - iteration\n",
            "loss = 0.3607002761384318\n",
            "accuracy = 0.916\n",
            "800 - iteration\n",
            "loss = 0.3476905778365773\n",
            "accuracy = 0.92\n",
            "900 - iteration\n",
            "loss = 0.335898584371309\n",
            "accuracy = 0.926\n",
            "1000 - iteration\n",
            "loss = 0.32502097967572985\n",
            "accuracy = 0.932\n",
            "Training for 31 - batch\n",
            "0 - iteration\n",
            "loss = 0.968568409123422\n",
            "accuracy = 0.696\n",
            "100 - iteration\n",
            "loss = 0.4659229680018501\n",
            "accuracy = 0.862\n",
            "200 - iteration\n",
            "loss = 0.4142859373810819\n",
            "accuracy = 0.886\n",
            "300 - iteration\n",
            "loss = 0.38229001859831413\n",
            "accuracy = 0.9\n",
            "400 - iteration\n",
            "loss = 0.3590306907272281\n",
            "accuracy = 0.906\n",
            "500 - iteration\n",
            "loss = 0.34040198847094894\n",
            "accuracy = 0.916\n",
            "600 - iteration\n",
            "loss = 0.3244921669242344\n",
            "accuracy = 0.924\n",
            "700 - iteration\n",
            "loss = 0.3103114350061444\n",
            "accuracy = 0.928\n",
            "800 - iteration\n",
            "loss = 0.29730223746502293\n",
            "accuracy = 0.942\n",
            "900 - iteration\n",
            "loss = 0.2851677663392701\n",
            "accuracy = 0.944\n",
            "1000 - iteration\n",
            "loss = 0.2738856284333518\n",
            "accuracy = 0.948\n",
            "Training for 32 - batch\n",
            "0 - iteration\n",
            "loss = 1.0614298700654017\n",
            "accuracy = 0.68\n",
            "100 - iteration\n",
            "loss = 0.502294138224631\n",
            "accuracy = 0.84\n",
            "200 - iteration\n",
            "loss = 0.43950415832147005\n",
            "accuracy = 0.868\n",
            "300 - iteration\n",
            "loss = 0.40240125102305796\n",
            "accuracy = 0.878\n",
            "400 - iteration\n",
            "loss = 0.37403979612151494\n",
            "accuracy = 0.9\n",
            "500 - iteration\n",
            "loss = 0.3511992254263192\n",
            "accuracy = 0.904\n",
            "600 - iteration\n",
            "loss = 0.3313100997751262\n",
            "accuracy = 0.912\n",
            "700 - iteration\n",
            "loss = 0.31373703688954196\n",
            "accuracy = 0.926\n",
            "800 - iteration\n",
            "loss = 0.2992714866593349\n",
            "accuracy = 0.932\n",
            "900 - iteration\n",
            "loss = 0.2869790058907915\n",
            "accuracy = 0.938\n",
            "1000 - iteration\n",
            "loss = 0.27597362929377756\n",
            "accuracy = 0.94\n",
            "Training for 33 - batch\n",
            "0 - iteration\n",
            "loss = 0.9789289687657257\n",
            "accuracy = 0.688\n",
            "100 - iteration\n",
            "loss = 0.42862916196076584\n",
            "accuracy = 0.888\n",
            "200 - iteration\n",
            "loss = 0.3709312671086258\n",
            "accuracy = 0.91\n",
            "300 - iteration\n",
            "loss = 0.3382810218265834\n",
            "accuracy = 0.928\n",
            "400 - iteration\n",
            "loss = 0.31345839120418034\n",
            "accuracy = 0.934\n",
            "500 - iteration\n",
            "loss = 0.29436951437214903\n",
            "accuracy = 0.938\n",
            "600 - iteration\n",
            "loss = 0.2788190932901356\n",
            "accuracy = 0.94\n",
            "700 - iteration\n",
            "loss = 0.26559285468614885\n",
            "accuracy = 0.944\n",
            "800 - iteration\n",
            "loss = 0.25401406618010514\n",
            "accuracy = 0.952\n",
            "900 - iteration\n",
            "loss = 0.24364503640681517\n",
            "accuracy = 0.954\n",
            "1000 - iteration\n",
            "loss = 0.23415974700740386\n",
            "accuracy = 0.956\n",
            "Training for 34 - batch\n",
            "0 - iteration\n",
            "loss = 1.0610321493477466\n",
            "accuracy = 0.662\n",
            "100 - iteration\n",
            "loss = 0.43698879890239517\n",
            "accuracy = 0.88\n",
            "200 - iteration\n",
            "loss = 0.38400653427889747\n",
            "accuracy = 0.908\n",
            "300 - iteration\n",
            "loss = 0.35010899880557167\n",
            "accuracy = 0.924\n",
            "400 - iteration\n",
            "loss = 0.32522531035796476\n",
            "accuracy = 0.936\n",
            "500 - iteration\n",
            "loss = 0.30577436134965613\n",
            "accuracy = 0.946\n",
            "600 - iteration\n",
            "loss = 0.2889385707554392\n",
            "accuracy = 0.95\n",
            "700 - iteration\n",
            "loss = 0.2740350843217651\n",
            "accuracy = 0.95\n",
            "800 - iteration\n",
            "loss = 0.261094414878711\n",
            "accuracy = 0.958\n",
            "900 - iteration\n",
            "loss = 0.24984403715991985\n",
            "accuracy = 0.958\n",
            "1000 - iteration\n",
            "loss = 0.23991299732053853\n",
            "accuracy = 0.958\n",
            "Training for 35 - batch\n",
            "0 - iteration\n",
            "loss = 1.1165082171845084\n",
            "accuracy = 0.66\n",
            "100 - iteration\n",
            "loss = 0.4988646699361862\n",
            "accuracy = 0.844\n",
            "200 - iteration\n",
            "loss = 0.3280599782204955\n",
            "accuracy = 0.914\n",
            "300 - iteration\n",
            "loss = 0.3015107412512894\n",
            "accuracy = 0.926\n",
            "400 - iteration\n",
            "loss = 0.28107695100692254\n",
            "accuracy = 0.936\n",
            "500 - iteration\n",
            "loss = 0.2621162645286271\n",
            "accuracy = 0.944\n",
            "600 - iteration\n",
            "loss = 0.24641262604172284\n",
            "accuracy = 0.948\n",
            "700 - iteration\n",
            "loss = 0.23435995846074764\n",
            "accuracy = 0.95\n",
            "800 - iteration\n",
            "loss = 0.22452340951932456\n",
            "accuracy = 0.948\n",
            "900 - iteration\n",
            "loss = 0.2158508490773899\n",
            "accuracy = 0.95\n",
            "1000 - iteration\n",
            "loss = 0.20801003608751764\n",
            "accuracy = 0.954\n",
            "Training for 36 - batch\n",
            "0 - iteration\n",
            "loss = 1.0537788425421435\n",
            "accuracy = 0.672\n",
            "100 - iteration\n",
            "loss = 0.3436678941130201\n",
            "accuracy = 0.91\n",
            "200 - iteration\n",
            "loss = 0.2963931336872456\n",
            "accuracy = 0.936\n",
            "300 - iteration\n",
            "loss = 0.2690898094293616\n",
            "accuracy = 0.94\n",
            "400 - iteration\n",
            "loss = 0.2497602371102717\n",
            "accuracy = 0.956\n",
            "500 - iteration\n",
            "loss = 0.23460099477848548\n",
            "accuracy = 0.958\n",
            "600 - iteration\n",
            "loss = 0.22259958914429104\n",
            "accuracy = 0.962\n",
            "700 - iteration\n",
            "loss = 0.21291373599479008\n",
            "accuracy = 0.964\n",
            "800 - iteration\n",
            "loss = 0.2047694285467921\n",
            "accuracy = 0.964\n",
            "900 - iteration\n",
            "loss = 0.19771533834019323\n",
            "accuracy = 0.964\n",
            "1000 - iteration\n",
            "loss = 0.1914740530495616\n",
            "accuracy = 0.964\n",
            "Training for 37 - batch\n",
            "0 - iteration\n",
            "loss = 0.9193762814170072\n",
            "accuracy = 0.696\n",
            "100 - iteration\n",
            "loss = 0.39962419661728926\n",
            "accuracy = 0.872\n",
            "200 - iteration\n",
            "loss = 0.33252707205041665\n",
            "accuracy = 0.904\n",
            "300 - iteration\n",
            "loss = 0.2976619865850309\n",
            "accuracy = 0.926\n",
            "400 - iteration\n",
            "loss = 0.27574960111214225\n",
            "accuracy = 0.946\n",
            "500 - iteration\n",
            "loss = 0.255163761327214\n",
            "accuracy = 0.958\n",
            "600 - iteration\n",
            "loss = 0.24043126982406113\n",
            "accuracy = 0.968\n",
            "700 - iteration\n",
            "loss = 0.22810373753180363\n",
            "accuracy = 0.97\n",
            "800 - iteration\n",
            "loss = 0.21731705311947047\n",
            "accuracy = 0.97\n",
            "900 - iteration\n",
            "loss = 0.2077368134278558\n",
            "accuracy = 0.972\n",
            "1000 - iteration\n",
            "loss = 0.1991474947479709\n",
            "accuracy = 0.972\n",
            "Training for 38 - batch\n",
            "0 - iteration\n",
            "loss = 1.0267867774477564\n",
            "accuracy = 0.674\n",
            "100 - iteration\n",
            "loss = 0.34497080729854873\n",
            "accuracy = 0.892\n",
            "200 - iteration\n",
            "loss = 0.2988704898159376\n",
            "accuracy = 0.922\n",
            "300 - iteration\n",
            "loss = 0.2719416420405538\n",
            "accuracy = 0.938\n",
            "400 - iteration\n",
            "loss = 0.25240346247230544\n",
            "accuracy = 0.94\n",
            "500 - iteration\n",
            "loss = 0.23697656678060008\n",
            "accuracy = 0.948\n",
            "600 - iteration\n",
            "loss = 0.2242916568216786\n",
            "accuracy = 0.95\n",
            "700 - iteration\n",
            "loss = 0.21356006184079968\n",
            "accuracy = 0.952\n",
            "800 - iteration\n",
            "loss = 0.20424536938500573\n",
            "accuracy = 0.956\n",
            "900 - iteration\n",
            "loss = 0.19597401763083702\n",
            "accuracy = 0.958\n",
            "1000 - iteration\n",
            "loss = 0.18848670437845202\n",
            "accuracy = 0.966\n",
            "Training for 39 - batch\n",
            "0 - iteration\n",
            "loss = 0.9812691989854638\n",
            "accuracy = 0.644\n",
            "100 - iteration\n",
            "loss = 0.34008066735382986\n",
            "accuracy = 0.912\n",
            "200 - iteration\n",
            "loss = 0.280746095403624\n",
            "accuracy = 0.942\n",
            "300 - iteration\n",
            "loss = 0.4101052757970109\n",
            "accuracy = 0.886\n",
            "400 - iteration\n",
            "loss = 0.2424598733374859\n",
            "accuracy = 0.966\n",
            "500 - iteration\n",
            "loss = 0.2293792761288967\n",
            "accuracy = 0.966\n",
            "600 - iteration\n",
            "loss = 0.21832687880007623\n",
            "accuracy = 0.968\n",
            "700 - iteration\n",
            "loss = 0.20866212869473283\n",
            "accuracy = 0.968\n",
            "800 - iteration\n",
            "loss = 0.20058548056504957\n",
            "accuracy = 0.972\n",
            "900 - iteration\n",
            "loss = 0.1934631801657272\n",
            "accuracy = 0.974\n",
            "1000 - iteration\n",
            "loss = 0.18698700158196463\n",
            "accuracy = 0.974\n",
            "Training for 40 - batch\n",
            "0 - iteration\n",
            "loss = 1.1013402584222869\n",
            "accuracy = 0.672\n",
            "100 - iteration\n",
            "loss = 0.3390395873843552\n",
            "accuracy = 0.91\n",
            "200 - iteration\n",
            "loss = 0.2944770133328313\n",
            "accuracy = 0.926\n",
            "300 - iteration\n",
            "loss = 0.2640853756767665\n",
            "accuracy = 0.944\n",
            "400 - iteration\n",
            "loss = 0.24532520252693416\n",
            "accuracy = 0.948\n",
            "500 - iteration\n",
            "loss = 0.23042360630344996\n",
            "accuracy = 0.948\n",
            "600 - iteration\n",
            "loss = 0.2176830307131253\n",
            "accuracy = 0.954\n",
            "700 - iteration\n",
            "loss = 0.20653294478852746\n",
            "accuracy = 0.956\n",
            "800 - iteration\n",
            "loss = 0.19661551037542793\n",
            "accuracy = 0.954\n",
            "900 - iteration\n",
            "loss = 0.1877191144043586\n",
            "accuracy = 0.96\n",
            "1000 - iteration\n",
            "loss = 0.17971233523201552\n",
            "accuracy = 0.966\n",
            "Training for 41 - batch\n",
            "0 - iteration\n",
            "loss = 0.6736545079345817\n",
            "accuracy = 0.778\n",
            "100 - iteration\n",
            "loss = 0.2773209814975386\n",
            "accuracy = 0.92\n",
            "200 - iteration\n",
            "loss = 0.22713471637127866\n",
            "accuracy = 0.938\n",
            "300 - iteration\n",
            "loss = 0.2023486110474482\n",
            "accuracy = 0.95\n",
            "400 - iteration\n",
            "loss = 0.18706566848922157\n",
            "accuracy = 0.954\n",
            "500 - iteration\n",
            "loss = 0.1748718978572607\n",
            "accuracy = 0.966\n",
            "600 - iteration\n",
            "loss = 0.1647371127817612\n",
            "accuracy = 0.974\n",
            "700 - iteration\n",
            "loss = 0.1560604510290015\n",
            "accuracy = 0.978\n",
            "800 - iteration\n",
            "loss = 0.14846949992540304\n",
            "accuracy = 0.982\n",
            "900 - iteration\n",
            "loss = 0.14171881293000296\n",
            "accuracy = 0.986\n",
            "1000 - iteration\n",
            "loss = 0.13564056746834566\n",
            "accuracy = 0.988\n",
            "Training for 42 - batch\n",
            "0 - iteration\n",
            "loss = 1.211622176741898\n",
            "accuracy = 0.654\n",
            "100 - iteration\n",
            "loss = 0.2725273321638489\n",
            "accuracy = 0.924\n",
            "200 - iteration\n",
            "loss = 0.2291776334902329\n",
            "accuracy = 0.948\n",
            "300 - iteration\n",
            "loss = 0.2062894832247781\n",
            "accuracy = 0.956\n",
            "400 - iteration\n",
            "loss = 0.1904165445793049\n",
            "accuracy = 0.964\n",
            "500 - iteration\n",
            "loss = 0.17815354553174045\n",
            "accuracy = 0.97\n",
            "600 - iteration\n",
            "loss = 0.16814077012305484\n",
            "accuracy = 0.974\n",
            "700 - iteration\n",
            "loss = 0.15969106442825054\n",
            "accuracy = 0.98\n",
            "800 - iteration\n",
            "loss = 0.15239926703651588\n",
            "accuracy = 0.984\n",
            "900 - iteration\n",
            "loss = 0.14600046813062406\n",
            "accuracy = 0.988\n",
            "1000 - iteration\n",
            "loss = 0.1403097713824442\n",
            "accuracy = 0.99\n",
            "Training for 43 - batch\n",
            "0 - iteration\n",
            "loss = 0.7998030184364175\n",
            "accuracy = 0.746\n",
            "100 - iteration\n",
            "loss = 0.24687635018089754\n",
            "accuracy = 0.942\n",
            "200 - iteration\n",
            "loss = 0.2099983899226728\n",
            "accuracy = 0.96\n",
            "300 - iteration\n",
            "loss = 0.1925922336496681\n",
            "accuracy = 0.968\n",
            "400 - iteration\n",
            "loss = 0.18056231736834633\n",
            "accuracy = 0.974\n",
            "500 - iteration\n",
            "loss = 0.17117927603642583\n",
            "accuracy = 0.976\n",
            "600 - iteration\n",
            "loss = 0.16340051987921275\n",
            "accuracy = 0.978\n",
            "700 - iteration\n",
            "loss = 0.15670372708744026\n",
            "accuracy = 0.982\n",
            "800 - iteration\n",
            "loss = 0.15079616670904278\n",
            "accuracy = 0.984\n",
            "900 - iteration\n",
            "loss = 0.14549837639622856\n",
            "accuracy = 0.984\n",
            "1000 - iteration\n",
            "loss = 0.14069179758848474\n",
            "accuracy = 0.984\n",
            "Training for 44 - batch\n",
            "0 - iteration\n",
            "loss = 1.045516328724864\n",
            "accuracy = 0.692\n",
            "100 - iteration\n",
            "loss = 0.22395173921876171\n",
            "accuracy = 0.95\n",
            "200 - iteration\n",
            "loss = 0.19365537872904695\n",
            "accuracy = 0.968\n",
            "300 - iteration\n",
            "loss = 0.18012860913723552\n",
            "accuracy = 0.976\n",
            "400 - iteration\n",
            "loss = 0.17030063307962431\n",
            "accuracy = 0.978\n",
            "500 - iteration\n",
            "loss = 0.16239980753355554\n",
            "accuracy = 0.98\n",
            "600 - iteration\n",
            "loss = 0.15571825734243533\n",
            "accuracy = 0.98\n",
            "700 - iteration\n",
            "loss = 0.14988897280642166\n",
            "accuracy = 0.98\n",
            "800 - iteration\n",
            "loss = 0.144694775664671\n",
            "accuracy = 0.982\n",
            "900 - iteration\n",
            "loss = 0.139995626812586\n",
            "accuracy = 0.984\n",
            "1000 - iteration\n",
            "loss = 0.13569547299292362\n",
            "accuracy = 0.986\n",
            "Training for 45 - batch\n",
            "0 - iteration\n",
            "loss = 0.8405117177266008\n",
            "accuracy = 0.74\n",
            "100 - iteration\n",
            "loss = 0.2269047571684942\n",
            "accuracy = 0.95\n",
            "200 - iteration\n",
            "loss = 0.1842281770700482\n",
            "accuracy = 0.964\n",
            "300 - iteration\n",
            "loss = 0.16409142024297105\n",
            "accuracy = 0.972\n",
            "400 - iteration\n",
            "loss = 0.15143771515852048\n",
            "accuracy = 0.98\n",
            "500 - iteration\n",
            "loss = 0.1420031671499623\n",
            "accuracy = 0.988\n",
            "600 - iteration\n",
            "loss = 0.13442279742981292\n",
            "accuracy = 0.99\n",
            "700 - iteration\n",
            "loss = 0.12804584277520592\n",
            "accuracy = 0.988\n",
            "800 - iteration\n",
            "loss = 0.12251683332150057\n",
            "accuracy = 0.988\n",
            "900 - iteration\n",
            "loss = 0.11762187422731389\n",
            "accuracy = 0.99\n",
            "1000 - iteration\n",
            "loss = 0.11322192524821816\n",
            "accuracy = 0.99\n",
            "Training for 46 - batch\n",
            "0 - iteration\n",
            "loss = 0.6597162068178722\n",
            "accuracy = 0.78\n",
            "100 - iteration\n",
            "loss = 0.2172806947819857\n",
            "accuracy = 0.95\n",
            "200 - iteration\n",
            "loss = 0.18760093910100387\n",
            "accuracy = 0.954\n",
            "300 - iteration\n",
            "loss = 0.16935456765012485\n",
            "accuracy = 0.962\n",
            "400 - iteration\n",
            "loss = 0.1559305426264705\n",
            "accuracy = 0.966\n",
            "500 - iteration\n",
            "loss = 0.1453639441384974\n",
            "accuracy = 0.964\n",
            "600 - iteration\n",
            "loss = 0.13673747822407073\n",
            "accuracy = 0.972\n",
            "700 - iteration\n",
            "loss = 0.12951450939621342\n",
            "accuracy = 0.972\n",
            "800 - iteration\n",
            "loss = 0.12333490845471354\n",
            "accuracy = 0.978\n",
            "900 - iteration\n",
            "loss = 0.11794618446158046\n",
            "accuracy = 0.978\n",
            "1000 - iteration\n",
            "loss = 0.11317026958906443\n",
            "accuracy = 0.98\n",
            "Training for 47 - batch\n",
            "0 - iteration\n",
            "loss = 0.9286791879279201\n",
            "accuracy = 0.716\n",
            "100 - iteration\n",
            "loss = 0.19747630294240465\n",
            "accuracy = 0.964\n",
            "200 - iteration\n",
            "loss = 0.1740047392937876\n",
            "accuracy = 0.966\n",
            "300 - iteration\n",
            "loss = 0.1601804978917655\n",
            "accuracy = 0.97\n",
            "400 - iteration\n",
            "loss = 0.14990765934575898\n",
            "accuracy = 0.972\n",
            "500 - iteration\n",
            "loss = 0.14026160479597763\n",
            "accuracy = 0.972\n",
            "600 - iteration\n",
            "loss = 0.132255822653867\n",
            "accuracy = 0.972\n",
            "700 - iteration\n",
            "loss = 0.12621510989917128\n",
            "accuracy = 0.974\n",
            "800 - iteration\n",
            "loss = 0.12095101164913716\n",
            "accuracy = 0.974\n",
            "900 - iteration\n",
            "loss = 0.11626394367808836\n",
            "accuracy = 0.978\n",
            "1000 - iteration\n",
            "loss = 0.11202830655484881\n",
            "accuracy = 0.978\n",
            "Training for 48 - batch\n",
            "0 - iteration\n",
            "loss = 0.6413238476486384\n",
            "accuracy = 0.79\n",
            "100 - iteration\n",
            "loss = 0.1628379107354511\n",
            "accuracy = 0.966\n",
            "200 - iteration\n",
            "loss = 0.14246884355555675\n",
            "accuracy = 0.972\n",
            "300 - iteration\n",
            "loss = 0.13005208750243458\n",
            "accuracy = 0.974\n",
            "400 - iteration\n",
            "loss = 0.12096867259355722\n",
            "accuracy = 0.976\n",
            "500 - iteration\n",
            "loss = 0.11385136356783492\n",
            "accuracy = 0.982\n",
            "600 - iteration\n",
            "loss = 0.10803010569871124\n",
            "accuracy = 0.984\n",
            "700 - iteration\n",
            "loss = 0.10311517149047233\n",
            "accuracy = 0.986\n",
            "800 - iteration\n",
            "loss = 0.09886467060096875\n",
            "accuracy = 0.988\n",
            "900 - iteration\n",
            "loss = 0.09512104030196963\n",
            "accuracy = 0.988\n",
            "1000 - iteration\n",
            "loss = 0.09177690747956725\n",
            "accuracy = 0.99\n",
            "Training for 49 - batch\n",
            "0 - iteration\n",
            "loss = 1.4388902763870137\n",
            "accuracy = 0.622\n",
            "100 - iteration\n",
            "loss = 0.22979593237881454\n",
            "accuracy = 0.94\n",
            "200 - iteration\n",
            "loss = 0.1752228773513742\n",
            "accuracy = 0.97\n",
            "300 - iteration\n",
            "loss = 0.15779202907054377\n",
            "accuracy = 0.982\n",
            "400 - iteration\n",
            "loss = 0.1465107555973575\n",
            "accuracy = 0.986\n",
            "500 - iteration\n",
            "loss = 0.13793862422261083\n",
            "accuracy = 0.986\n",
            "600 - iteration\n",
            "loss = 0.130942497274435\n",
            "accuracy = 0.986\n",
            "700 - iteration\n",
            "loss = 0.12501279014560912\n",
            "accuracy = 0.988\n",
            "800 - iteration\n",
            "loss = 0.11987208001979821\n",
            "accuracy = 0.988\n",
            "900 - iteration\n",
            "loss = 0.11534413453682987\n",
            "accuracy = 0.988\n",
            "1000 - iteration\n",
            "loss = 0.11130574488916167\n",
            "accuracy = 0.988\n",
            "Training for 50 - batch\n",
            "0 - iteration\n",
            "loss = 0.8529593222401499\n",
            "accuracy = 0.744\n",
            "100 - iteration\n",
            "loss = 0.18453797987763867\n",
            "accuracy = 0.952\n",
            "200 - iteration\n",
            "loss = 0.16319408808180427\n",
            "accuracy = 0.968\n",
            "300 - iteration\n",
            "loss = 0.15036135628392833\n",
            "accuracy = 0.978\n",
            "400 - iteration\n",
            "loss = 0.14114023749714244\n",
            "accuracy = 0.98\n",
            "500 - iteration\n",
            "loss = 0.1339179492118891\n",
            "accuracy = 0.98\n",
            "600 - iteration\n",
            "loss = 0.1279510753031949\n",
            "accuracy = 0.98\n",
            "700 - iteration\n",
            "loss = 0.12284228402904207\n",
            "accuracy = 0.982\n",
            "800 - iteration\n",
            "loss = 0.11835609132855596\n",
            "accuracy = 0.984\n",
            "900 - iteration\n",
            "loss = 0.11434180230288249\n",
            "accuracy = 0.984\n",
            "1000 - iteration\n",
            "loss = 0.11069734851923106\n",
            "accuracy = 0.984\n",
            "Training for 51 - batch\n",
            "0 - iteration\n",
            "loss = 0.765748797724025\n",
            "accuracy = 0.78\n",
            "100 - iteration\n",
            "loss = 0.14019822965508508\n",
            "accuracy = 0.98\n",
            "200 - iteration\n",
            "loss = 0.1173390717251021\n",
            "accuracy = 0.99\n",
            "300 - iteration\n",
            "loss = 0.10505497353132284\n",
            "accuracy = 0.992\n",
            "400 - iteration\n",
            "loss = 0.09698377467693502\n",
            "accuracy = 0.994\n",
            "500 - iteration\n",
            "loss = 0.091048804110295\n",
            "accuracy = 0.996\n",
            "600 - iteration\n",
            "loss = 0.08634619418796305\n",
            "accuracy = 0.996\n",
            "700 - iteration\n",
            "loss = 0.08243499757157997\n",
            "accuracy = 0.996\n",
            "800 - iteration\n",
            "loss = 0.07907498412658777\n",
            "accuracy = 0.996\n",
            "900 - iteration\n",
            "loss = 0.07612203282840818\n",
            "accuracy = 0.996\n",
            "1000 - iteration\n",
            "loss = 0.07348292057531752\n",
            "accuracy = 0.996\n",
            "Training for 52 - batch\n",
            "0 - iteration\n",
            "loss = 0.9009105655309897\n",
            "accuracy = 0.73\n",
            "100 - iteration\n",
            "loss = 0.17370521495870817\n",
            "accuracy = 0.952\n",
            "200 - iteration\n",
            "loss = 0.14682225884921238\n",
            "accuracy = 0.956\n",
            "300 - iteration\n",
            "loss = 0.13253325972090693\n",
            "accuracy = 0.966\n",
            "400 - iteration\n",
            "loss = 0.12254854352677425\n",
            "accuracy = 0.974\n",
            "500 - iteration\n",
            "loss = 0.11489269559460459\n",
            "accuracy = 0.982\n",
            "600 - iteration\n",
            "loss = 0.1087344075513158\n",
            "accuracy = 0.982\n",
            "700 - iteration\n",
            "loss = 0.10359679051088516\n",
            "accuracy = 0.986\n",
            "800 - iteration\n",
            "loss = 0.09918376916732068\n",
            "accuracy = 0.986\n",
            "900 - iteration\n",
            "loss = 0.09530784496445363\n",
            "accuracy = 0.992\n",
            "1000 - iteration\n",
            "loss = 0.09184598071977235\n",
            "accuracy = 0.992\n",
            "Training for 53 - batch\n",
            "0 - iteration\n",
            "loss = 1.2065617170931169\n",
            "accuracy = 0.682\n",
            "100 - iteration\n",
            "loss = 0.1550738561172655\n",
            "accuracy = 0.97\n",
            "200 - iteration\n",
            "loss = 0.13557928972200586\n",
            "accuracy = 0.978\n",
            "300 - iteration\n",
            "loss = 0.12402660829310057\n",
            "accuracy = 0.986\n",
            "400 - iteration\n",
            "loss = 0.11582492581261365\n",
            "accuracy = 0.988\n",
            "500 - iteration\n",
            "loss = 0.1094539091103904\n",
            "accuracy = 0.988\n",
            "600 - iteration\n",
            "loss = 0.1042373446942405\n",
            "accuracy = 0.99\n",
            "700 - iteration\n",
            "loss = 0.09981573637480119\n",
            "accuracy = 0.99\n",
            "800 - iteration\n",
            "loss = 0.09597612923025259\n",
            "accuracy = 0.99\n",
            "900 - iteration\n",
            "loss = 0.09258246455560501\n",
            "accuracy = 0.99\n",
            "1000 - iteration\n",
            "loss = 0.08954252387773909\n",
            "accuracy = 0.99\n",
            "Training for 54 - batch\n",
            "0 - iteration\n",
            "loss = 0.5954222440696849\n",
            "accuracy = 0.792\n",
            "100 - iteration\n",
            "loss = 0.12297002938904773\n",
            "accuracy = 0.974\n",
            "200 - iteration\n",
            "loss = 0.10234574969915057\n",
            "accuracy = 0.986\n",
            "300 - iteration\n",
            "loss = 0.0904385110263703\n",
            "accuracy = 0.99\n",
            "400 - iteration\n",
            "loss = 0.08251228041306882\n",
            "accuracy = 0.992\n",
            "500 - iteration\n",
            "loss = 0.07667888331377791\n",
            "accuracy = 0.994\n",
            "600 - iteration\n",
            "loss = 0.07208277420954613\n",
            "accuracy = 0.996\n",
            "700 - iteration\n",
            "loss = 0.068302335613079\n",
            "accuracy = 0.996\n",
            "800 - iteration\n",
            "loss = 0.0651022736576864\n",
            "accuracy = 0.998\n",
            "900 - iteration\n",
            "loss = 0.062336774638331774\n",
            "accuracy = 0.998\n",
            "1000 - iteration\n",
            "loss = 0.05990846012211944\n",
            "accuracy = 0.998\n",
            "Training for 55 - batch\n",
            "0 - iteration\n",
            "loss = 1.318187066799618\n",
            "accuracy = 0.666\n",
            "100 - iteration\n",
            "loss = 0.7823659288620854\n",
            "accuracy = 0.794\n",
            "200 - iteration\n",
            "loss = 0.13167513064591344\n",
            "accuracy = 0.978\n",
            "300 - iteration\n",
            "loss = 0.1100689465000181\n",
            "accuracy = 0.98\n",
            "400 - iteration\n",
            "loss = 0.09804303950563421\n",
            "accuracy = 0.986\n",
            "500 - iteration\n",
            "loss = 0.08982150019917712\n",
            "accuracy = 0.99\n",
            "600 - iteration\n",
            "loss = 0.08363627991055204\n",
            "accuracy = 0.99\n",
            "700 - iteration\n",
            "loss = 0.07872637649763412\n",
            "accuracy = 0.992\n",
            "800 - iteration\n",
            "loss = 0.07463616542727748\n",
            "accuracy = 0.996\n",
            "900 - iteration\n",
            "loss = 0.07112602434485654\n",
            "accuracy = 0.998\n",
            "1000 - iteration\n",
            "loss = 0.0680602888395236\n",
            "accuracy = 0.998\n",
            "Training for 56 - batch\n",
            "0 - iteration\n",
            "loss = 0.53477886431818\n",
            "accuracy = 0.814\n",
            "100 - iteration\n",
            "loss = 0.14073041720861693\n",
            "accuracy = 0.972\n",
            "200 - iteration\n",
            "loss = 0.11813192290543101\n",
            "accuracy = 0.982\n",
            "300 - iteration\n",
            "loss = 0.1081758629040915\n",
            "accuracy = 0.986\n",
            "400 - iteration\n",
            "loss = 0.10147674596414345\n",
            "accuracy = 0.986\n",
            "500 - iteration\n",
            "loss = 0.0963545114392069\n",
            "accuracy = 0.986\n",
            "600 - iteration\n",
            "loss = 0.0921804383420555\n",
            "accuracy = 0.988\n",
            "700 - iteration\n",
            "loss = 0.08864510791963726\n",
            "accuracy = 0.988\n",
            "800 - iteration\n",
            "loss = 0.08557130362773807\n",
            "accuracy = 0.988\n",
            "900 - iteration\n",
            "loss = 0.08284739950049387\n",
            "accuracy = 0.988\n",
            "1000 - iteration\n",
            "loss = 0.08039832336794119\n",
            "accuracy = 0.988\n",
            "Training for 57 - batch\n",
            "0 - iteration\n",
            "loss = 1.0632088028395306\n",
            "accuracy = 0.712\n",
            "100 - iteration\n",
            "loss = 0.10382872448586081\n",
            "accuracy = 0.986\n",
            "200 - iteration\n",
            "loss = 0.09079585021601778\n",
            "accuracy = 0.992\n",
            "300 - iteration\n",
            "loss = 0.0825981037663529\n",
            "accuracy = 0.992\n",
            "400 - iteration\n",
            "loss = 0.07664278088002278\n",
            "accuracy = 0.994\n",
            "500 - iteration\n",
            "loss = 0.0719999608790421\n",
            "accuracy = 0.994\n",
            "600 - iteration\n",
            "loss = 0.06822387944797571\n",
            "accuracy = 0.996\n",
            "700 - iteration\n",
            "loss = 0.06506677415893805\n",
            "accuracy = 0.996\n",
            "800 - iteration\n",
            "loss = 0.062373438295789056\n",
            "accuracy = 0.996\n",
            "900 - iteration\n",
            "loss = 0.060037682374073524\n",
            "accuracy = 0.996\n",
            "1000 - iteration\n",
            "loss = 0.05798311912650104\n",
            "accuracy = 0.996\n",
            "Training for 58 - batch\n",
            "0 - iteration\n",
            "loss = 0.6117223327704651\n",
            "accuracy = 0.8\n",
            "100 - iteration\n",
            "loss = 0.10909903771175436\n",
            "accuracy = 0.986\n",
            "200 - iteration\n",
            "loss = 0.09394054925002374\n",
            "accuracy = 0.994\n",
            "300 - iteration\n",
            "loss = 0.08534844150148371\n",
            "accuracy = 0.994\n",
            "400 - iteration\n",
            "loss = 0.07940591107046925\n",
            "accuracy = 0.994\n",
            "500 - iteration\n",
            "loss = 0.07487256151713759\n",
            "accuracy = 0.994\n",
            "600 - iteration\n",
            "loss = 0.07120744086318298\n",
            "accuracy = 0.994\n",
            "700 - iteration\n",
            "loss = 0.06812831240283436\n",
            "accuracy = 0.994\n",
            "800 - iteration\n",
            "loss = 0.06546963071220883\n",
            "accuracy = 0.996\n",
            "900 - iteration\n",
            "loss = 0.06312606812794554\n",
            "accuracy = 0.996\n",
            "1000 - iteration\n",
            "loss = 0.06102657999252451\n",
            "accuracy = 0.996\n",
            "Training for 59 - batch\n",
            "0 - iteration\n",
            "loss = 0.883367277901032\n",
            "accuracy = 0.734\n",
            "100 - iteration\n",
            "loss = 0.1160275437693656\n",
            "accuracy = 0.978\n",
            "200 - iteration\n",
            "loss = 0.09903239060972902\n",
            "accuracy = 0.982\n",
            "300 - iteration\n",
            "loss = 0.08932905180505321\n",
            "accuracy = 0.988\n",
            "400 - iteration\n",
            "loss = 0.08241170326629373\n",
            "accuracy = 0.99\n",
            "500 - iteration\n",
            "loss = 0.07699556069045169\n",
            "accuracy = 0.992\n",
            "600 - iteration\n",
            "loss = 0.0726386736427413\n",
            "accuracy = 0.996\n",
            "700 - iteration\n",
            "loss = 0.06907716450932591\n",
            "accuracy = 0.998\n",
            "800 - iteration\n",
            "loss = 0.06608301697539308\n",
            "accuracy = 0.998\n",
            "900 - iteration\n",
            "loss = 0.0634953741871641\n",
            "accuracy = 0.998\n",
            "1000 - iteration\n",
            "loss = 0.06121091858002711\n",
            "accuracy = 1.0\n",
            "Training for 60 - batch\n",
            "0 - iteration\n",
            "loss = 0.6370872504059871\n",
            "accuracy = 0.776\n",
            "100 - iteration\n",
            "loss = 0.09692335198548116\n",
            "accuracy = 0.984\n",
            "200 - iteration\n",
            "loss = 0.08115585497279501\n",
            "accuracy = 0.99\n",
            "300 - iteration\n",
            "loss = 0.0726085366829484\n",
            "accuracy = 0.992\n",
            "400 - iteration\n",
            "loss = 0.06666997971587134\n",
            "accuracy = 0.994\n",
            "500 - iteration\n",
            "loss = 0.06212362143445971\n",
            "accuracy = 0.994\n",
            "600 - iteration\n",
            "loss = 0.0584591645976489\n",
            "accuracy = 0.996\n",
            "700 - iteration\n",
            "loss = 0.05541014879332667\n",
            "accuracy = 0.998\n",
            "800 - iteration\n",
            "loss = 0.05281613254845395\n",
            "accuracy = 0.998\n",
            "900 - iteration\n",
            "loss = 0.050570846289554446\n",
            "accuracy = 0.998\n",
            "1000 - iteration\n",
            "loss = 0.04859955040269756\n",
            "accuracy = 0.998\n",
            "Training for 61 - batch\n",
            "0 - iteration\n",
            "loss = 0.6995424871952209\n",
            "accuracy = 0.792\n",
            "100 - iteration\n",
            "loss = 0.11409687802881964\n",
            "accuracy = 0.978\n",
            "200 - iteration\n",
            "loss = 0.0936956222629908\n",
            "accuracy = 0.984\n",
            "300 - iteration\n",
            "loss = 0.08304885255239768\n",
            "accuracy = 0.99\n",
            "400 - iteration\n",
            "loss = 0.07613727829435545\n",
            "accuracy = 0.99\n",
            "500 - iteration\n",
            "loss = 0.07112430020088584\n",
            "accuracy = 0.994\n",
            "600 - iteration\n",
            "loss = 0.06721766237643159\n",
            "accuracy = 0.996\n",
            "700 - iteration\n",
            "loss = 0.06401773141581042\n",
            "accuracy = 0.998\n",
            "800 - iteration\n",
            "loss = 0.061302645755469815\n",
            "accuracy = 0.998\n",
            "900 - iteration\n",
            "loss = 0.05893977356166323\n",
            "accuracy = 0.998\n",
            "1000 - iteration\n",
            "loss = 0.05684479860514854\n",
            "accuracy = 0.998\n",
            "Training for 62 - batch\n",
            "0 - iteration\n",
            "loss = 0.9494085101985344\n",
            "accuracy = 0.738\n",
            "100 - iteration\n",
            "loss = 0.08379622074891287\n",
            "accuracy = 0.994\n",
            "200 - iteration\n",
            "loss = 0.06965268162326911\n",
            "accuracy = 0.996\n",
            "300 - iteration\n",
            "loss = 0.06142344461358639\n",
            "accuracy = 1.0\n",
            "400 - iteration\n",
            "loss = 0.05576987270328024\n",
            "accuracy = 1.0\n",
            "500 - iteration\n",
            "loss = 0.05153795532554992\n",
            "accuracy = 1.0\n",
            "600 - iteration\n",
            "loss = 0.04820980072558227\n",
            "accuracy = 1.0\n",
            "700 - iteration\n",
            "loss = 0.0455113695526406\n",
            "accuracy = 1.0\n",
            "800 - iteration\n",
            "loss = 0.04327347627396086\n",
            "accuracy = 1.0\n",
            "900 - iteration\n",
            "loss = 0.04137974020049612\n",
            "accuracy = 1.0\n",
            "1000 - iteration\n",
            "loss = 0.03974777245846321\n",
            "accuracy = 1.0\n",
            "Training for 63 - batch\n",
            "0 - iteration\n",
            "loss = 0.716828821964792\n",
            "accuracy = 0.764\n",
            "100 - iteration\n",
            "loss = 0.0855666867969421\n",
            "accuracy = 0.992\n",
            "200 - iteration\n",
            "loss = 0.07466410366316757\n",
            "accuracy = 0.996\n",
            "300 - iteration\n",
            "loss = 0.06817792976393253\n",
            "accuracy = 0.998\n",
            "400 - iteration\n",
            "loss = 0.06351086716626986\n",
            "accuracy = 0.998\n",
            "500 - iteration\n",
            "loss = 0.05988095037977432\n",
            "accuracy = 0.998\n",
            "600 - iteration\n",
            "loss = 0.05693374314006337\n",
            "accuracy = 0.998\n",
            "700 - iteration\n",
            "loss = 0.054467612100721846\n",
            "accuracy = 0.998\n",
            "800 - iteration\n",
            "loss = 0.05235533559079926\n",
            "accuracy = 0.998\n",
            "900 - iteration\n",
            "loss = 0.05051196704226844\n",
            "accuracy = 0.998\n",
            "1000 - iteration\n",
            "loss = 0.04887841722753679\n",
            "accuracy = 0.998\n",
            "Training for 64 - batch\n",
            "0 - iteration\n",
            "loss = 0.7739188810176804\n",
            "accuracy = 0.762\n",
            "100 - iteration\n",
            "loss = 0.07477732910924673\n",
            "accuracy = 0.992\n",
            "200 - iteration\n",
            "loss = 0.06304424288235853\n",
            "accuracy = 0.994\n",
            "300 - iteration\n",
            "loss = 0.05664541052402983\n",
            "accuracy = 0.996\n",
            "400 - iteration\n",
            "loss = 0.05231887266686899\n",
            "accuracy = 0.996\n",
            "500 - iteration\n",
            "loss = 0.049068424253720475\n",
            "accuracy = 0.996\n",
            "600 - iteration\n",
            "loss = 0.04647173225617077\n",
            "accuracy = 0.996\n",
            "700 - iteration\n",
            "loss = 0.04431216139346246\n",
            "accuracy = 0.998\n",
            "800 - iteration\n",
            "loss = 0.042464302542105134\n",
            "accuracy = 0.998\n",
            "900 - iteration\n",
            "loss = 0.040849442368597874\n",
            "accuracy = 0.998\n",
            "1000 - iteration\n",
            "loss = 0.03941525329307731\n",
            "accuracy = 0.998\n",
            "Training for 65 - batch\n",
            "0 - iteration\n",
            "loss = 0.5884518162258507\n",
            "accuracy = 0.81\n",
            "100 - iteration\n",
            "loss = 0.06895548172065881\n",
            "accuracy = 0.994\n",
            "200 - iteration\n",
            "loss = 0.060661107296584735\n",
            "accuracy = 0.994\n",
            "300 - iteration\n",
            "loss = 0.05561154530381991\n",
            "accuracy = 0.994\n",
            "400 - iteration\n",
            "loss = 0.052009440891216\n",
            "accuracy = 0.994\n",
            "500 - iteration\n",
            "loss = 0.049274526252596824\n",
            "accuracy = 0.996\n",
            "600 - iteration\n",
            "loss = 0.04708977621962377\n",
            "accuracy = 0.998\n",
            "700 - iteration\n",
            "loss = 0.04527342264693969\n",
            "accuracy = 0.998\n",
            "800 - iteration\n",
            "loss = 0.04371915379423395\n",
            "accuracy = 0.998\n",
            "900 - iteration\n",
            "loss = 0.04236041812804474\n",
            "accuracy = 0.998\n",
            "1000 - iteration\n",
            "loss = 0.04115278885766071\n",
            "accuracy = 0.998\n",
            "Training for 66 - batch\n",
            "0 - iteration\n",
            "loss = 0.6363763666468749\n",
            "accuracy = 0.814\n",
            "100 - iteration\n",
            "loss = 0.07290461103445385\n",
            "accuracy = 0.992\n",
            "200 - iteration\n",
            "loss = 0.06246101977255601\n",
            "accuracy = 0.998\n",
            "300 - iteration\n",
            "loss = 0.05638917954429105\n",
            "accuracy = 0.998\n",
            "400 - iteration\n",
            "loss = 0.052158870502960274\n",
            "accuracy = 0.998\n",
            "500 - iteration\n",
            "loss = 0.048956290675312764\n",
            "accuracy = 0.998\n",
            "600 - iteration\n",
            "loss = 0.046403219620507655\n",
            "accuracy = 0.998\n",
            "700 - iteration\n",
            "loss = 0.04429299573009599\n",
            "accuracy = 1.0\n",
            "800 - iteration\n",
            "loss = 0.04250154619827893\n",
            "accuracy = 1.0\n",
            "900 - iteration\n",
            "loss = 0.04094922902308808\n",
            "accuracy = 1.0\n",
            "1000 - iteration\n",
            "loss = 0.039582187788295656\n",
            "accuracy = 1.0\n",
            "Training for 67 - batch\n",
            "0 - iteration\n",
            "loss = 0.5265920219239467\n",
            "accuracy = 0.824\n",
            "100 - iteration\n",
            "loss = 0.08351799825782359\n",
            "accuracy = 0.99\n",
            "200 - iteration\n",
            "loss = 0.07156153067579078\n",
            "accuracy = 0.994\n",
            "300 - iteration\n",
            "loss = 0.0645671794747171\n",
            "accuracy = 0.996\n",
            "400 - iteration\n",
            "loss = 0.059618756956768194\n",
            "accuracy = 0.998\n",
            "500 - iteration\n",
            "loss = 0.05581085001758089\n",
            "accuracy = 0.998\n",
            "600 - iteration\n",
            "loss = 0.05273191817380116\n",
            "accuracy = 1.0\n",
            "700 - iteration\n",
            "loss = 0.05016007327443577\n",
            "accuracy = 1.0\n",
            "800 - iteration\n",
            "loss = 0.047961237009636046\n",
            "accuracy = 1.0\n",
            "900 - iteration\n",
            "loss = 0.046047674821428575\n",
            "accuracy = 1.0\n",
            "1000 - iteration\n",
            "loss = 0.044358678074736146\n",
            "accuracy = 1.0\n",
            "Training for 68 - batch\n",
            "0 - iteration\n",
            "loss = 0.5720439213286655\n",
            "accuracy = 0.832\n",
            "100 - iteration\n",
            "loss = 0.07298952542960963\n",
            "accuracy = 0.988\n",
            "200 - iteration\n",
            "loss = 0.061223695693033535\n",
            "accuracy = 0.994\n",
            "300 - iteration\n",
            "loss = 0.054862987290666035\n",
            "accuracy = 0.994\n",
            "400 - iteration\n",
            "loss = 0.050535073101472226\n",
            "accuracy = 0.996\n",
            "500 - iteration\n",
            "loss = 0.04728630778280876\n",
            "accuracy = 0.998\n",
            "600 - iteration\n",
            "loss = 0.04470831667311823\n",
            "accuracy = 1.0\n",
            "700 - iteration\n",
            "loss = 0.042585896584808824\n",
            "accuracy = 1.0\n",
            "800 - iteration\n",
            "loss = 0.04079108756918255\n",
            "accuracy = 1.0\n",
            "900 - iteration\n",
            "loss = 0.0392416636635491\n",
            "accuracy = 1.0\n",
            "1000 - iteration\n",
            "loss = 0.03788180699151943\n",
            "accuracy = 1.0\n",
            "Training for 69 - batch\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "list index out of range",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-199-2c29d0027c8a>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_normalized\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Training for {j} - batch'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m   \u001b[0mz_0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_normalized\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0ml1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgradient_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0ml1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0ml1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0ml2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0ml2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0ml3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0ml3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mY_batches_ohe\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mz_3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mz_1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mz_2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mz_0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: list index out of range"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tcNd1ZTLd_Tl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}